{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## 1.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf8\n",
    "import warnings\n",
    "\n",
    "\n",
    "class DefaultConfig(object):\n",
    "    env = 'UNetResNet'  # visdom 环境\n",
    "    model = 'UNetResNet'  # 使用的模型，名字必须与models/__init__.py中的名字一致\n",
    "    ori_data_dir = '../input'\n",
    "    train_data_images_root = '../input/train/images'  # 训练集存放路径\n",
    "    train_data_masks_root = '../input/train/masks'  # 训练集mask存放路径\n",
    "    test_data_root = '../input/test/images'  # 测试集存放路径\n",
    "    load_model_path = None  # 加载预训练的模型的路径，为None代表不加载\n",
    "\n",
    "    # 原始图片大小\n",
    "    ori_image_h = 101\n",
    "    ori_image_w = 101\n",
    "    ori_image_channels = 3\n",
    "    # 图片的输入\n",
    "    image_h = 128\n",
    "    image_w = 128\n",
    "    image_channels = 3\n",
    "\n",
    "    img_size_ori = 101\n",
    "    img_size_target = 128\n",
    "\n",
    "    batch_size = 32  # batch size\n",
    "    use_gpu = True  # user GPU or not\n",
    "    num_workers = 1  # how many workers for loading data\n",
    "    print_freq = 20  # print info every N batch\n",
    "    pretrained = True\n",
    "    useValBestThred = True\n",
    "    BestThred = 0.5\n",
    "    valPercent = 0.2\n",
    "\n",
    "    debug_file = '/tmp/debug'  # if os.path.exists(debug_file): enter ipdb\n",
    "    result_file = 'result.csv'\n",
    "\n",
    "    max_epoch = 50\n",
    "    lr = 0.01  # initial learning rate\n",
    "    lr_decay = 0.95  # when val_loss increase, lr = lr*lr_decay\n",
    "    weight_decay = 0e-4  # 损失函数\n",
    "\n",
    "    seed = 1126\n",
    "\n",
    "    # Loss\n",
    "    dice_weight =  5.0\n",
    "    bce_weight = 1.0\n",
    "\n",
    "    num_folds = 5\n",
    "    stratified = True\n",
    "    \n",
    "    SIZE = 128\n",
    "    PAD  = 16\n",
    "    \n",
    "    fold_iter_num = 1\n",
    "\n",
    "def parse(self, kwargs):\n",
    "    \"\"\"\n",
    "    根据字典kwargs 更新 config参数\n",
    "    \"\"\"\n",
    "    for k, v in kwargs.items():\n",
    "        if not hasattr(self, k):\n",
    "            warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
    "        setattr(self, k, v)\n",
    "\n",
    "    print('user config:')\n",
    "    for k, v in self.__class__.__dict__.items():\n",
    "        if not k.startswith('__'):\n",
    "            print(k, getattr(self, k))\n",
    "\n",
    "DefaultConfig.parse = parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "0fbc8b8703a832b2d2d83f1c8fa4f53ec81b5466",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = DefaultConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d174710457805582a644350c3fdf6441343b87d4"
   },
   "source": [
    "## 2.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "850f5bee71c439e6189946489d739370593addea"
   },
   "source": [
    "### 2.1 Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "219cb242e4dbf02dfc674407b02fb2e55159e2ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf8\n",
    "import torch as t\n",
    "import time\n",
    "\n",
    "\n",
    "class BasicModule(t.nn.Module):\n",
    "    \"\"\"\n",
    "    封装了nn.Module,主要是提供了save和load两个方法\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BasicModule,self).__init__()\n",
    "        self.model_name=str(type(self))# 默认名字\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        可加载指定路径的模型\n",
    "        \"\"\"\n",
    "        self.load_state_dict(t.load(path))\n",
    "\n",
    "    def save(self, name=None):\n",
    "        \"\"\"\n",
    "        保存模型，默认使用“模型名字+时间”作为文件名\n",
    "        \"\"\"\n",
    "        if name is None:\n",
    "            prefix = 'checkpoints/' + self.model_name + '_'\n",
    "            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
    "        t.save(self.state_dict(), name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class Flat(t.nn.Module):\n",
    "    \"\"\"\n",
    "    把输入reshape成（batch_size,dim_length）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flat, self).__init__()\n",
    "        #self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "797d565266f0d16355122ef2b8d9e290c061b966"
   },
   "source": [
    "###  2.2 UNetResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d624c8f1f7fdeb072dadc7427dd8b977916ca3bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NoOperation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DecoderBlockV2(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlockV2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UNetResNet(BasicModule):\n",
    "    \"\"\"PyTorch U-Net model using ResNet(34, 101 or 152) encoder.\n",
    "\n",
    "    UNet: https://arxiv.org/abs/1505.04597\n",
    "    ResNet: https://arxiv.org/abs/1512.03385\n",
    "    Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
    "\n",
    "    Args:\n",
    "            encoder_depth (int): Depth of a ResNet encoder (34, 101 or 152).\n",
    "            num_classes (int): Number of output classes.\n",
    "            num_filters (int, optional): Number of filters in the last layer of decoder. Defaults to 32.\n",
    "            dropout_2d (float, optional): Probability factor of dropout layer before output layer. Defaults to 0.2.\n",
    "            pretrained (bool, optional):\n",
    "                False - no pre-trained weights are being used.\n",
    "                True  - ResNet encoder is pre-trained on ImageNet.\n",
    "                Defaults to False.\n",
    "            is_deconv (bool, optional):\n",
    "                False: bilinear interpolation is used in decoder.\n",
    "                True: deconvolution is used in decoder.\n",
    "                Defaults to False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_depth, num_classes = 1, num_filters=32, dropout_2d=0.2,\n",
    "                 pretrained=False, is_deconv=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_2d = dropout_2d\n",
    "\n",
    "        if encoder_depth == 34:\n",
    "            self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n",
    "            bottom_channel_nr = 512\n",
    "        elif encoder_depth == 101:\n",
    "            self.encoder = torchvision.models.resnet101(pretrained=pretrained)\n",
    "            bottom_channel_nr = 2048\n",
    "        elif encoder_depth == 152:\n",
    "            self.encoder = torchvision.models.resnet152(pretrained=pretrained)\n",
    "            bottom_channel_nr = 2048\n",
    "        else:\n",
    "            raise NotImplementedError('only 34, 101, 152 version of Resnet are implemented')\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
    "                                   self.encoder.bn1,\n",
    "                                   self.encoder.relu,\n",
    "                                   self.pool)\n",
    "\n",
    "        self.conv2 = self.encoder.layer1\n",
    "\n",
    "        self.conv3 = self.encoder.layer2\n",
    "\n",
    "        self.conv4 = self.encoder.layer3\n",
    "\n",
    "        self.conv5 = self.encoder.layer4\n",
    "\n",
    "        self.center = DecoderBlockV2(bottom_channel_nr, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec5 = DecoderBlockV2(bottom_channel_nr + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(bottom_channel_nr // 2 + num_filters * 8, num_filters * 8 * 2, num_filters * 8,\n",
    "                                   is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(bottom_channel_nr // 4 + num_filters * 8, num_filters * 4 * 2, num_filters * 2,\n",
    "                                   is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2,\n",
    "                                   is_deconv)\n",
    "        self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec0 = ConvRelu(num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        pool = self.pool(conv5)\n",
    "        center = self.center(pool)\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(dec2)\n",
    "        dec0 = self.dec0(dec1)\n",
    "\n",
    "        return F.sigmoid(torch.squeeze(self.final(F.dropout2d(dec0, p=self.dropout_2d))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbacaecad28b393590184bfaee780de2d214bb72"
   },
   "source": [
    "### 2.3 UNetResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b1f11595da26b07c9fdd9ffa1993ccce29566a05",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ConvBn2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)):\n",
    "        super(ConvBn2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        #self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.bn = SynchronizedBatchNorm2d(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv(z)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, channels, out_channels ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 =  ConvBn2d(in_channels,  channels, kernel_size=3, padding=1)\n",
    "        self.conv2 =  ConvBn2d(channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x ):\n",
    "        x = F.upsample(x, scale_factor=2, mode='bilinear')#False\n",
    "        x = F.relu(self.conv1(x),inplace=True)\n",
    "        x = F.relu(self.conv2(x),inplace=True)\n",
    "        return x\n",
    "\n",
    "#\n",
    "# resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n",
    "# resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth'\n",
    "# resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth'\n",
    "# resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'\n",
    "# resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth'\n",
    "\n",
    "class UNetResNet34(BasicModule):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.resnet = torchvision.models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            self.resnet.conv1,\n",
    "            self.resnet.bn1,\n",
    "            self.resnet.relu,\n",
    "        )# 64\n",
    "        self.encoder2 = self.resnet.layer1  # 64\n",
    "        self.encoder3 = self.resnet.layer2  #128\n",
    "        self.encoder4 = self.resnet.layer3  #256\n",
    "        self.encoder5 = self.resnet.layer4  #512\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            ConvBn2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ConvBn2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder5 = Decoder(512+256, 512, 256)\n",
    "        self.decoder4 = Decoder(256+256, 512, 256)\n",
    "        self.decoder3 = Decoder(128+256, 256,  64)\n",
    "        self.decoder2 = Decoder( 64+ 64, 128, 128)\n",
    "        self.decoder1 = Decoder(128    , 128,  32)\n",
    "\n",
    "        self.logit    = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32,  1, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #batch_size,C,H,W = x.shape\n",
    "\n",
    "        mean=[0.485, 0.456, 0.406]\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "        x = torch.stack([\n",
    "            (x-mean[0])/std[0],\n",
    "            (x-mean[1])/std[1],\n",
    "            (x-mean[2])/std[2],\n",
    "        ],1)\n",
    "\n",
    "        #print(x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        e2 = self.encoder2( x)  #; print('e2',e2.size())\n",
    "        e3 = self.encoder3(e2)  #; print('e3',e3.size())\n",
    "        e4 = self.encoder4(e3)  #; print('e4',e4.size())\n",
    "        e5 = self.encoder5(e4)  #; print('e5',e5.size())\n",
    "\n",
    "\n",
    "        #f = F.max_pool2d(e5, kernel_size=2, stride=2 )  #; print(f.size())\n",
    "        #f = F.upsample(f, scale_factor=2, mode='bilinear', align_corners=True)#False\n",
    "        #f = self.center(f)                       #; print('center',f.size())\n",
    "        f = self.center(e5)\n",
    "         \n",
    "        f = self.decoder5(torch.cat([f, e5], 1))  #; print('d5',f.size())\n",
    "        f = self.decoder4(torch.cat([f, e4], 1))  #; print('d4',f.size())\n",
    "        f = self.decoder3(torch.cat([f, e3], 1))  #; print('d3',f.size())\n",
    "        f = self.decoder2(torch.cat([f, e2], 1))  #; print('d2',f.size())\n",
    "        f = self.decoder1(f)                      # ; print('d1',f.size())\n",
    "\n",
    "        #f = F.dropout2d(f, p=0.20)\n",
    "        logit = self.logit(f)                     #; print('logit',logit.size())\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "449340416e2fc00ed35761cbb83f174110cf71cb"
   },
   "source": [
    "### 2.4 UNetSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "395186e5b2a7c70255d40d508cecfe4c7b051241",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ConvBn2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(ConvBn2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv(z)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "## net  ######################################################################\n",
    "class UNetSimple(BasicModule):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(UNetSimple, self).__init__()\n",
    "\n",
    "        self.down1 = nn.Sequential(\n",
    "            ConvBn2d(1,  64, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(64,  64, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            ConvBn2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            ConvBn2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            ConvBn2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.down5 = nn.Sequential(\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.same = nn.Sequential(\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.up5 = nn.Sequential(\n",
    "            ConvBn2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.up4 = nn.Sequential(\n",
    "            ConvBn2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "            ConvBn2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            ConvBn2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(128,  64, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.up1 = nn.Sequential(\n",
    "            ConvBn2d(128,  64, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBn2d(64,  64, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.feature = nn.Sequential(\n",
    "            ConvBn2d(64,  64, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "        self.logit = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.unsqueeze(input,1)\n",
    "        down1 = self.down1(input)\n",
    "        # , return_indices=True)\n",
    "        f = F.max_pool2d(down1, kernel_size=2, stride=2)\n",
    "        down2 = self.down2(f)\n",
    "        f = F.max_pool2d(down2, kernel_size=2, stride=2)\n",
    "        down3 = self.down3(f)\n",
    "        f = F.max_pool2d(down3, kernel_size=2, stride=2)\n",
    "        down4 = self.down4(f)\n",
    "        f = F.max_pool2d(down4, kernel_size=2, stride=2)\n",
    "        down5 = self.down5(f)\n",
    "        f = F.max_pool2d(down5, kernel_size=2, stride=2)\n",
    "\n",
    "        f = self.same(f)\n",
    "\n",
    "        f = F.upsample(f, scale_factor=2, mode='bilinear')\n",
    "        #f = F.max_unpool2d(f, i4, kernel_size=2, stride=2)\n",
    "        f = self.up5(torch.cat([down5, f], 1))\n",
    "\n",
    "        f = F.upsample(f, scale_factor=2, mode='bilinear')\n",
    "        f = self.up4(torch.cat([down4, f], 1))\n",
    "\n",
    "        f = F.upsample(f, scale_factor=2, mode='bilinear')\n",
    "        f = self.up3(torch.cat([down3, f], 1))\n",
    "\n",
    "        f = F.upsample(f, scale_factor=2, mode='bilinear')\n",
    "        f = self.up2(torch.cat([down2, f], 1))\n",
    "\n",
    "        f = F.upsample(f, scale_factor=2, mode='bilinear')\n",
    "        f = self.up1(torch.cat([down1, f], 1))\n",
    "\n",
    "        f = self.feature(f)\n",
    "        #f = F.dropout(f, p=0.5)\n",
    "        logit = self.logit(f)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff8e130ddaf554b22a7d86ae8033849d2acfe4f6"
   },
   "source": [
    "## 3. utilus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5555d8fa6c12b6875c94ae50f0709434cffa185f"
   },
   "source": [
    "### 3.1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "6bc090d391b6062845e04289abe6280704546392",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf8\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import random\n",
    "\n",
    "\n",
    "class TGSSaltDataSet(data.Dataset):\n",
    "    def __init__(self, DataNameDict, opt, mode='train', train_transforms=None, test_transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        主要目标： 获取所有图片的地址，并根据训练，验证，测试划分数据\n",
    "        \"\"\"\n",
    "        self.opt = opt\n",
    "        self.mode = mode\n",
    "        self.imgs = DataNameDict[self.mode]\n",
    "\n",
    "        if train_transforms is None:\n",
    "            normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "            self.train_transforms = T.Compose([\n",
    "                T.Resize(self.opt.image_h),\n",
    "                T.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transforms = train_transforms\n",
    "        if target_transform is None:\n",
    "            self.target_transform = T.Compose([\n",
    "                T.Resize(self.opt.image_h),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        一次返回一张图片的数据\n",
    "        \"\"\"\n",
    "        img_name = self.imgs[index]\n",
    "        if self.mode == 'test':\n",
    "            img_path = os.path.join(\n",
    "                self.opt.test_data_root, '%s.png' % img_name)\n",
    "            data = Image.open(img_path)\n",
    "            data = self.train_transforms(data)\n",
    "            return data, img_name\n",
    "        else:\n",
    "            img_path_train = os.path.join(\n",
    "                self.opt.train_data_images_root, '%s.png' % img_name)\n",
    "            img_path_mask = os.path.join(\n",
    "                self.opt.train_data_masks_root, '%s.png' % img_name)\n",
    "            data = Image.open(img_path_train)\n",
    "            data = self.train_transforms(data)\n",
    "            mask = Image.open(img_path_mask)\n",
    "            mask = self.target_transform(mask)\n",
    "            mask = (mask > 128).float()\n",
    "            mask = torch.squeeze(mask)\n",
    "            return data, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "class TrainDataSet(data.Dataset):\n",
    "    def __init__(self, train_ids, opt):\n",
    "        \"\"\"\n",
    "        主要目标： 获取所有图片的地址，并根据训练，验证，测试划分数据\n",
    "        \"\"\"\n",
    "        self.opt = opt\n",
    "        self.imgs = train_ids\n",
    "        self.train_transforms = T.Compose([\n",
    "            T.Resize(self.opt.image_h),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.target_transform = T.Compose([\n",
    "            T.Resize(self.opt.image_h),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        一次返回一张图片的数据\n",
    "        \"\"\"\n",
    "        img_name = self.imgs[index]\n",
    "        data = train_df.loc[img_name, 'images']\n",
    "        mask = train_df.loc[img_name, 'masks']\n",
    "        #data, mask = np.asarray(data), np.asarray(mask)\n",
    "        data, mask = self.train_augment(data, mask)\n",
    "        data, mask = torch.from_numpy(data), torch.from_numpy(mask)\n",
    "        return data, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def train_augment(self, image, mask):\n",
    "    \n",
    "        if np.random.rand() < 0.5:\n",
    "            image, mask = do_horizontal_flip2(image, mask)\n",
    "    \n",
    "        if np.random.rand() < 0.5:\n",
    "            c = np.random.choice(4)\n",
    "            if c == 0:\n",
    "                image, mask = do_random_shift_scale_crop_pad2(\n",
    "                    image, mask, 0.2)  # 0.125\n",
    "    \n",
    "            if c == 1:\n",
    "                image, mask = do_horizontal_shear2(\n",
    "                    image, mask, dx=np.random.uniform(-0.07, 0.07))\n",
    "                pass\n",
    "    \n",
    "            if c == 2:\n",
    "                image, mask = do_shift_scale_rotate2(\n",
    "                    image, mask, dx=0, dy=0, scale=1, angle=np.random.uniform(0, 15))  # 10\n",
    "    \n",
    "            if c == 3:\n",
    "                image, mask = do_elastic_transform2(\n",
    "                    image, mask, grid=10, distort=np.random.uniform(0, 0.15))  # 0.10\n",
    "    \n",
    "        if np.random.rand() < 0.5:\n",
    "            c = np.random.choice(3)\n",
    "            if c == 0:\n",
    "                image = do_brightness_shift(image, np.random.uniform(-0.1, +0.1))\n",
    "            if c == 1:\n",
    "                image = do_brightness_multiply(\n",
    "                    image, np.random.uniform(1-0.08, 1+0.08))\n",
    "            if c == 2:\n",
    "                image = do_gamma(image, np.random.uniform(1-0.08, 1+0.08))\n",
    "    \n",
    "        #image, mask = do_resize2(image, mask, self.opt.SIZE, self.opt.SIZE)\n",
    "        #image, mask = do_center_pad2(image, mask, self.opt.PAD)\n",
    "        image, mask = do_center_pad_to_factor2(image, mask, factor=32)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class ValDataSet(data.Dataset):\n",
    "    def __init__(self, val_ids, opt):\n",
    "        \"\"\"\n",
    "        主要目标： 获取所有图片的地址，并根据训练，验证，测试划分数据\n",
    "        \"\"\"\n",
    "        self.opt = opt\n",
    "        self.imgs = val_ids\n",
    "        self.train_transforms = T.Compose([\n",
    "            T.Resize(self.opt.image_h),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.target_transform = T.Compose([\n",
    "            T.Resize(self.opt.image_h),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        一次返回一张图片的数据\n",
    "        \"\"\"\n",
    "        img_name = self.imgs[index]\n",
    "        data = train_df.loc[img_name, 'images']\n",
    "        mask = train_df.loc[img_name, 'masks']\n",
    "        #data, mask = np.asarray(data), np.asarray(mask)\n",
    "        data, mask = self.val_augment(data, mask)\n",
    "        data, mask = torch.from_numpy(data), torch.from_numpy(mask)\n",
    "        #mask = torch.squeeze(mask)\n",
    "        return data, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def val_augment(self, image, mask):\n",
    "        #image, mask = do_resize2(image, mask, self.opt.SIZE, self.opt.SIZE)\n",
    "        #image, mask = do_center_pad2(image, mask, self.opt.PAD)\n",
    "        image, mask = do_center_pad_to_factor2(image, mask, factor=32)\n",
    "        return image,mask\n",
    "\n",
    "class TestDataSet(data.Dataset):\n",
    "    def __init__(self, test_ids, opt):\n",
    "        \"\"\"\n",
    "        主要目标： 获取所有图片的地址，并根据训练，验证，测试划分数据\n",
    "        \"\"\"\n",
    "        self.opt = opt\n",
    "        self.imgs = test_ids\n",
    "        self.train_transforms = T.Compose([\n",
    "            T.Resize(self.opt.image_h),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        一次返回一张图片的数据\n",
    "        \"\"\"\n",
    "        img_name = self.imgs[index]\n",
    "        data = test_df.loc[img_name, 'images']\n",
    "        #data = np.asarray(data)\n",
    "        data = self.test_augment(data)\n",
    "        data = torch.from_numpy(data)\n",
    "        return data, img_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def test_augment(self, image):\n",
    "        #image = do_resize(image, self.opt.SIZE, self.opt.SIZE)\n",
    "        #image = do_center_pad(image, self.opt.PAD)\n",
    "        image = do_center_pad_to_factor(image, factor=32)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c00798684e81f24bca0a9e8d156fc2ea4003a425"
   },
   "source": [
    "### 3.2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e2d668a07edd7506076379bd8e741ca71d5235da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=0, eps=1e-7):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        output = torch.squeeze(output)\n",
    "        target = torch.squeeze(target)\n",
    "        return 1 - (2 * torch.sum(output * target) + self.smooth) / (\n",
    "            torch.sum(output) + torch.sum(target) + self.smooth + self.eps)\n",
    "\n",
    "\n",
    "class mixed_dice_bce_loss(nn.Module):\n",
    "    \"\"\"docstring for ClassName\"\"\"\n",
    "\n",
    "    def __init__(self, dice_weight=0.2, bce_weight=0.9, smooth=0):\n",
    "        super(mixed_dice_bce_loss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        bce_loss = nn.BCEWithLogitsLoss()\n",
    "        dice_loss = DiceLoss(smooth=self.smooth)\n",
    "        return self.dice_weight * dice_loss(output, target) + self.bce_weight * bce_loss(output, target)\n",
    "\n",
    "class FocalLoss2d(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, size_average=True):\n",
    "        super(FocalLoss2d, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, logit, target, class_weight=None, type='sigmoid'):\n",
    "        target = target.view(-1, 1).long()\n",
    "\n",
    "        if type == 'sigmoid':\n",
    "            if class_weight is None:\n",
    "                class_weight = [1]*2  # [0.5, 0.5]\n",
    "\n",
    "            prob = F.sigmoid(logit)\n",
    "            prob = prob.view(-1, 1)\n",
    "            prob = torch.cat((1-prob, prob), 1)\n",
    "            select = torch.FloatTensor(len(prob), 2).zero_().cuda()\n",
    "            select.scatter_(1, target.data, 1.)\n",
    "\n",
    "        elif type == 'softmax':\n",
    "            B, C, H, W = logit.size()\n",
    "            if class_weight is None:\n",
    "                class_weight = [1]*C  # [1/C]*C\n",
    "\n",
    "            logit = logit.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "            prob = F.softmax(logit, 1)\n",
    "            select = torch.FloatTensor(len(prob), C).zero_().cuda()\n",
    "            select.scatter_(1, target, 1.)\n",
    "\n",
    "        class_weight = torch.FloatTensor(class_weight).cuda().view(-1, 1)\n",
    "        class_weight = torch.gather(class_weight, 0, target.data)\n",
    "\n",
    "        prob = (prob*select).sum(1).view(-1, 1)\n",
    "        prob = torch.clamp(prob, 1e-8, 1-1e-8)\n",
    "        batch_loss = - class_weight * \\\n",
    "            (torch.pow((1-prob), self.gamma))*prob.log()\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# http://geek.csdn.net/news/detail/126833\n",
    "class PseudoBCELoss2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PseudoBCELoss2d, self).__init__()\n",
    "\n",
    "    def forward(self, logit, truth):\n",
    "        z = logit.view(-1)\n",
    "        t = truth.view(-1)\n",
    "        loss = z.clamp(min=0) - z*t + torch.log(1 + torch.exp(-z.abs()))\n",
    "        loss = loss.sum()/len(t)  # w.sum()\n",
    "        return loss\n",
    "\n",
    "class RobustFocalLoss2d(nn.Module):\n",
    "    # assume top 10% is outliers\n",
    "    def __init__(self, gamma=2, size_average=True):\n",
    "        super(RobustFocalLoss2d, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, logit, target, class_weight=None, type='sigmoid'):\n",
    "        target = target.view(-1, 1).long()\n",
    "\n",
    "        if type == 'sigmoid':\n",
    "            if class_weight is None:\n",
    "                class_weight = [1]*2  # [0.5, 0.5]\n",
    "\n",
    "            prob = F.sigmoid(logit)\n",
    "            prob = prob.view(-1, 1)\n",
    "            prob = torch.cat((1-prob, prob), 1)\n",
    "            select = Variable(torch.FloatTensor(len(prob), 2).zero_()).cuda()\n",
    "            select.scatter_(1, target, 1.)\n",
    "\n",
    "        elif type == 'softmax':\n",
    "            B, C, H, W = logit.size()\n",
    "            if class_weight is None:\n",
    "                class_weight = [1]*C  # [1/C]*C\n",
    "\n",
    "            logit = logit.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "            prob = F.softmax(logit, 1)\n",
    "            select = torch.FloatTensor(len(prob), C).zero_().cuda()\n",
    "            select.scatter_(1, target, 1.)\n",
    "\n",
    "        class_weight = Variable(torch.FloatTensor(class_weight)).cuda().view(-1, 1)\n",
    "        class_weight = torch.gather(class_weight, 0, target)\n",
    "\n",
    "        prob = (prob*select).sum(1).view(-1, 1)\n",
    "        prob = torch.clamp(prob, 1e-8, 1-1e-8)\n",
    "\n",
    "        focus = torch.pow((1-prob), self.gamma)\n",
    "        #focus = torch.where(focus < 2.0, focus, torch.zeros(prob.size()).cuda())\n",
    "        focus = torch.clamp(focus, 0, 2)\n",
    "\n",
    "        batch_loss = - class_weight * focus*prob.log()\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c565bb3e33865edf0e5748ac1d44326a18e83282"
   },
   "source": [
    "### 3.3 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "68a135867ae61c07c53e3c766f9ea3bcd8f8c75d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Score the model and do a threshold optimization by the best IoU.\n",
    "\n",
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "\n",
    "\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "\n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(),\n",
    "                           bins=([0, 0.5, 1], [0, 0.5, 1]))\n",
    "    intersection = temp1[0]\n",
    "    area_true = np.histogram(labels, bins=[0, 0.5, 1])[0]\n",
    "    area_pred = np.histogram(y_pred, bins=[0, 0.5, 1])[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:, 1:]\n",
    "    intersection[intersection == 0] = 1e-9\n",
    "\n",
    "    union = union[1:, 1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(\n",
    "            false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "\n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in, thred=0.5):\n",
    "    y_pred_in = np.int32(y_pred_in > thred)  # added by sgx 20180728\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)\n",
    "\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return iou_metric_batch(label, pred)\n",
    "\n",
    "\n",
    "def GetBestThred(y_valid_ori, preds_valid):\n",
    "    thresholds = np.arange(0.3,0.75,0.05)\n",
    "    ious = np.array([iou_metric_batch(y_valid_ori, \n",
    "        preds_valid, threshold) for threshold in thresholds])\n",
    "    maxindex = np.argmax(ious)\n",
    "    bestThred, bestIOU = thresholds[maxindex], ious[maxindex]\n",
    "    return bestThred, bestIOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66aa42d86e817529cb3a34f84d29d175c5cfdfdd"
   },
   "source": [
    "### 3.4 utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "42b1a952a4ff5e73718d460a4156985df90549fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import BaseCrossValidator, train_test_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def GenDataNameDict():\n",
    "    np.random.seed(opt.seed)\n",
    "    DataNameDict = {}\n",
    "    AllTrainNameList = [trainname.split(\n",
    "        '.')[-2] for trainname in os.listdir(os.path.join(opt.train_data_images_root))]\n",
    "    np.random.shuffle(AllTrainNameList)\n",
    "    AllTestNameList = [testname.split(\n",
    "        '.')[-2] for testname in os.listdir(os.path.join(opt.test_data_root))]\n",
    "    ValNameList = AllTrainNameList[:int(\n",
    "        opt.valPercent * len(AllTrainNameList))]  # 取百分之二十作为验证集\n",
    "    TrainNameList = AllTrainNameList[int(\n",
    "        opt.valPercent * len(AllTrainNameList)):]\n",
    "    DataNameDict['train'] = TrainNameList\n",
    "    DataNameDict['val'] = ValNameList\n",
    "    DataNameDict['test'] = AllTestNameList\n",
    "    return DataNameDict\n",
    "\n",
    "\n",
    "def CalMeanIOU(score, target):\n",
    "    metric_by_threshold = []\n",
    "    for threshold in np.linspace(0, 1, 11):\n",
    "        val_binary_prediction = (score > threshold).astype(int)\n",
    "\n",
    "        iou_values = []\n",
    "        for i in range(score.shape[0]):\n",
    "            y_mask = target[i]\n",
    "            p_mask = val_binary_prediction[i]\n",
    "            iou = jaccard_similarity_score(y_mask.flatten(), p_mask.flatten())\n",
    "            iou_values.append(iou)\n",
    "        iou_values = np.array(iou_values)\n",
    "\n",
    "        accuracies = [\n",
    "            np.mean(iou_values > iou_threshold)\n",
    "            for iou_threshold in np.linspace(0.5, 0.95, 10)\n",
    "        ]\n",
    "        metric_by_threshold.append((np.mean(accuracies), threshold))\n",
    "\n",
    "    best_metric, best_threshold = max(metric_by_threshold)\n",
    "    return best_metric, best_threshold\n",
    "\n",
    "\n",
    "def write_csv(results, file_name):\n",
    "    import csv\n",
    "    with open(file_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'rle_mask'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "\n",
    "def run_length_encoding(x):\n",
    "    # https://www.kaggle.com/c/data-science-bowl-2018/discussion/48561#\n",
    "    bs = np.where(x.T.flatten())[0]\n",
    "\n",
    "    rle = []\n",
    "    prev = -2\n",
    "    for b in bs:\n",
    "        if (b > prev + 1):\n",
    "            rle.extend((b + 1, 0))\n",
    "        rle[-1] += 1\n",
    "        prev = b\n",
    "    return rle\n",
    "\n",
    "\n",
    "def encode_rle(predictions):\n",
    "    return [run_length_encoding(mask) for mask in predictions]\n",
    "\n",
    "\n",
    "def create_submission(predictions_name, predictions, opt, thred=0.5):\n",
    "    output = []\n",
    "    for image_id, mask in zip(predictions_name, predictions):\n",
    "        mask = resize(np.squeeze(mask),\n",
    "                      (opt.ori_image_h, opt.ori_image_w),\n",
    "                      mode='constant', preserve_range=True)\n",
    "        mask = (mask > thred).astype(np.uint8)\n",
    "        rle_encoded = ' '.join(str(rle) for rle in run_length_encoding(mask))\n",
    "        output.append([str(image_id), rle_encoded])\n",
    "\n",
    "    submission = pd.DataFrame(output, columns=['id', 'rle_mask']).astype(str)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def get_mean_rle_mask(test_df):\n",
    "    pred_fold_list = test_df.filter(regex='thred_pred_fold*').columns.tolist()\n",
    "    for index in test_df.index:\n",
    "        mask = np.squeeze((np.sum(test_df.loc[index, pred_fold_list].values, axis=0)\n",
    "                           > (len(pred_fold_list)/2)).astype(np.uint8))\n",
    "        mask = resize(np.squeeze(mask),\n",
    "                      (opt.ori_image_h, opt.ori_image_w),\n",
    "                      mode='constant', preserve_range=True)\n",
    "        test_df.loc[index, 'rle_mask'] = ' '.join(\n",
    "            str(rle) for rle in run_length_encoding(mask))\n",
    "    return test_df\n",
    "\n",
    "def get_fold_rle_mask(test_df, fold_num):\n",
    "    pred_fold_list = test_df.filter(regex='thred_pred_fold%d*'%fold_num).columns.tolist()\n",
    "    for index in test_df.index:\n",
    "        mask = resize(np.squeeze(test_df.loc[index,pred_fold_list]),\n",
    "                      (opt.ori_image_h, opt.ori_image_w),\n",
    "                      mode='constant', preserve_range=True)\n",
    "        test_df.loc[index, 'rle_mask'] = ' '.join(\n",
    "            str(rle) for rle in run_length_encoding(mask))\n",
    "    return test_df\n",
    "\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    \"\"\"Resize image to target size\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Image of shape (C x H x W).\n",
    "        target_size (tuple): Target size (H, W).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized image of shape (C x H x W).\n",
    "\n",
    "    \"\"\"\n",
    "    n_channels = image.shape[0]\n",
    "    resized_image = resize(\n",
    "        image, (n_channels, target_size[0], target_size[1]), mode='constant')\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def get_crop_pad_sequence(vertical, horizontal):\n",
    "    top = int(vertical / 2)\n",
    "    bottom = vertical - top\n",
    "    right = int(horizontal / 2)\n",
    "    left = horizontal - right\n",
    "    return (top, right, bottom, left)\n",
    "\n",
    "\n",
    "def crop_image(image, target_size):\n",
    "    \"\"\"Crop image to target size. Image cropped symmetrically.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Image of shape (C x H x W).\n",
    "        target_size (tuple): Target size (H, W).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Cropped image of shape (C x H x W).\n",
    "\n",
    "    \"\"\"\n",
    "    top_crop, right_crop, bottom_crop, left_crop = get_crop_pad_sequence(image.shape[1] - target_size[0],\n",
    "                                                                         image.shape[2] - target_size[1])\n",
    "    cropped_image = image[:, top_crop:image.shape[1] -\n",
    "                          bottom_crop, left_crop:image.shape[2] - right_crop]\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def binarize(image, threshold):\n",
    "    image_binarized = (image[1, :, :] > threshold).astype(np.uint8)\n",
    "    return image_binarized\n",
    "\n",
    "\n",
    "def run_length_decoding(mask_rle, shape):\n",
    "    \"\"\"\n",
    "    Based on https://www.kaggle.com/msl23518/visualize-the-stage1-test-solution and modified\n",
    "    Args:\n",
    "        mask_rle: run-length as string formatted (start length)\n",
    "        shape: (height, width) of array to return\n",
    "\n",
    "    Returns:\n",
    "        numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    \"\"\"\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int)\n",
    "                       for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[1] * shape[0], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape((shape[1], shape[0])).T\n",
    "\n",
    "\n",
    "def upsample(img):\n",
    "    if opt.img_size_ori == opt.img_size_target:\n",
    "        return img\n",
    "    return resize(img, (opt.img_size_target, opt.img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    # return res\n",
    "\n",
    "\n",
    "def downsample(img):\n",
    "    if opt.img_size_ori == opt.img_size_target:\n",
    "        return img\n",
    "    return resize(img, (opt.img_size_ori, opt.img_size_ori), mode='constant', preserve_range=True)\n",
    "\n",
    "\n",
    "def GenTrainTest_df():\n",
    "    train_df = pd.read_csv(os.path.join(\n",
    "        opt.ori_data_dir, \"train.csv\"), index_col=\"id\", usecols=[0])\n",
    "    depths_df = pd.read_csv(os.path.join(\n",
    "        opt.ori_data_dir, \"depths.csv\"), index_col=\"id\")\n",
    "    train_df = train_df.join(depths_df)\n",
    "    test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "    train_df[\"images\"] = [Image.open(os.path.join(\n",
    "        opt.train_data_images_root, '%s.png' % idx)) for idx in train_df.index]\n",
    "    test_df[\"images\"] = [Image.open(os.path.join(\n",
    "        opt.test_data_root, '%s.png' % idx)) for idx in test_df.index]\n",
    "    train_df[\"masks\"] = [Image.open(os.path.join(\n",
    "        opt.train_data_masks_root, '%s.png' % idx)) for idx in train_df.index]\n",
    "    train_df[\"coverage\"] = train_df.masks.apply(lambda x:np.sum(np.array(x)>128)/(opt.ori_image_h**2))\n",
    "\n",
    "    def cov_to_class(val):\n",
    "        for i in range(0, 11):\n",
    "            if val * 10 <= i:\n",
    "                return i\n",
    "    train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "    return test_df, train_df\n",
    "\n",
    "def GenTrainTest_df_cv2():\n",
    "    train_df = pd.read_csv(os.path.join(\n",
    "        opt.ori_data_dir, \"train.csv\"), index_col=\"id\", usecols=[0])\n",
    "    depths_df = pd.read_csv(os.path.join(\n",
    "        opt.ori_data_dir, \"depths.csv\"), index_col=\"id\")\n",
    "    train_df = train_df.join(depths_df)\n",
    "    test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "    train_df[\"images\"] = [cv2.imread(os.path.join(\n",
    "        opt.train_data_images_root, '%s.png' % idx),cv2.IMREAD_GRAYSCALE).astype(np.float32)/255 for idx in train_df.index]\n",
    "    test_df[\"images\"] = [cv2.imread(os.path.join(\n",
    "        opt.test_data_root, '%s.png' % idx),cv2.IMREAD_GRAYSCALE).astype(np.float32)/255 for idx in test_df.index]\n",
    "    train_df[\"masks\"] = [cv2.imread(os.path.join(\n",
    "        opt.train_data_masks_root, '%s.png' % idx),cv2.IMREAD_GRAYSCALE).astype(np.float32)/255 for idx in train_df.index]\n",
    "    train_df[\"coverage\"] = train_df.masks.apply(lambda x:np.sum(x>0.5)/(x.shape[0]*x.shape[1]))\n",
    "\n",
    "    def cov_to_class(val):\n",
    "        for i in range(0, 11):\n",
    "            if val * 10 <= i:\n",
    "                return i\n",
    "    train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "    return test_df, train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b55c322b77ba84cab5aa48ab657c4e3614a42aa"
   },
   "source": [
    "### 3.5 augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4e86eb6baafc86298796772a4c2c80d84a40bf5f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def train_df_aug(train_df, aub_num=5000):\n",
    "    train_df_new = pd.DataFrame(columns=train_df.columns.tolist())\n",
    "    train_df_new['images'] = train_df_new['images'].astype(object)\n",
    "    train_df_new['masks'] = train_df_new['masks'].astype(object)\n",
    "    for i in range(5):\n",
    "        train_df_small = train_df[(train_df.z > i*200) & (train_df.z < (1+i)*200)]\n",
    "        count = 0\n",
    "        for id1, id2 in combinations(train_df_small.index.tolist(),2):\n",
    "            print(count)\n",
    "            count += 1\n",
    "            img1 = np.asarray(train_df_small.loc[id1, 'images'])\n",
    "            img2 = np.asarray(train_df_small.loc[id2, 'images'])\n",
    "            mask1 = np.asarray(train_df_small.loc[id1, 'masks'])\n",
    "            mask2 = np.asarray(train_df_small.loc[id2, 'masks'])\n",
    "            id_new = id1+'_'+id2\n",
    "            img_new = np.zeros(img1.shape)\n",
    "            mask_new = np.zeros(mask1.shape)\n",
    "            z_new = 0\n",
    "            coverage_new = 0\n",
    "            coverage_class_new = 0\n",
    "            mask_union = np.logical_or(mask1,mask2)\n",
    "            mask_new[mask_union] = 255\n",
    "            alpha = np.random.rand()\n",
    "            img_new[np.tile(~mask_union[:,:,np.newaxis],(1,1,3))] = alpha*img1[np.tile(~mask_union[:,:,np.newaxis],(1,1,3))] + (1-alpha)*img2[np.tile(~mask_union[:,:,np.newaxis],(1,1,3))]\n",
    "            img_new[np.tile(np.logical_and(mask_union,mask1)[:,:,np.newaxis],(1,1,3))] = img1[np.tile(np.logical_and(mask_union,mask1)[:,:,np.newaxis],(1,1,3))]\n",
    "            img_new[np.tile(np.logical_and(mask_union,mask2)[:,:,np.newaxis],(1,1,3))] = img2[np.tile(np.logical_and(mask_union,mask2)[:,:,np.newaxis],(1,1,3))]\n",
    "            if np.sum(np.logical_and(mask_union,mask1))>np.sum(np.logical_and(mask_union,mask2)):  \n",
    "                z_new = train_df_small.loc[id1,'z']\n",
    "                coverage_new = train_df_small.loc[id1,'coverage']\n",
    "                coverage_class_new = train_df_small.loc[id1,'coverage_class']\n",
    "            else:\n",
    "                z_new = train_df_small.loc[id2,'z']\n",
    "                coverage_new = train_df_small.loc[id2,'coverage']\n",
    "                coverage_class_new = train_df_small.loc[id2,'coverage_class']\n",
    "            train_df_new.loc[id_new, 'z'] = z_new\n",
    "            train_df_new.loc[id_new, 'coverage'] = coverage_new\n",
    "            train_df_new.loc[id_new, 'coverage_class'] = coverage_class_new\n",
    "            train_df_new.loc[id_new, 'images'] = Image.fromarray(img_new.astype(np.uint8))\n",
    "            train_df_new.loc[id_new, 'masks'] = Image.fromarray(mask_new.astype(np.uint8))\n",
    "            if count>1000:\n",
    "                break\n",
    "    train_df = train_df.append(train_df_new)\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6bf210288b2a8d52d57ec58cf6890dabf763d89"
   },
   "source": [
    "### 3.6 comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f914cad2290037d213a81d8509c2ee231cec53b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# File   : comm.py\n",
    "# Author : Jiayuan Mao\n",
    "# Email  : maojiayuan@gmail.com\n",
    "# Date   : 27/01/2018\n",
    "#\n",
    "# This file is part of Synchronized-BatchNorm-PyTorch.\n",
    "# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
    "# Distributed under MIT License.\n",
    "\n",
    "import queue\n",
    "import collections\n",
    "import threading\n",
    "\n",
    "__all__ = ['FutureResult', 'SlavePipe', 'SyncMaster']\n",
    "\n",
    "\n",
    "class FutureResult(object):\n",
    "    \"\"\"A thread-safe future implementation. Used only as one-to-one pipe.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._result = None\n",
    "        self._lock = threading.Lock()\n",
    "        self._cond = threading.Condition(self._lock)\n",
    "\n",
    "    def put(self, result):\n",
    "        with self._lock:\n",
    "            assert self._result is None, 'Previous result has\\'t been fetched.'\n",
    "            self._result = result\n",
    "            self._cond.notify()\n",
    "\n",
    "    def get(self):\n",
    "        with self._lock:\n",
    "            if self._result is None:\n",
    "                self._cond.wait()\n",
    "\n",
    "            res = self._result\n",
    "            self._result = None\n",
    "            return res\n",
    "\n",
    "\n",
    "_MasterRegistry = collections.namedtuple('MasterRegistry', ['result'])\n",
    "_SlavePipeBase = collections.namedtuple('_SlavePipeBase', ['identifier', 'queue', 'result'])\n",
    "\n",
    "\n",
    "class SlavePipe(_SlavePipeBase):\n",
    "    \"\"\"Pipe for master-slave communication.\"\"\"\n",
    "\n",
    "    def run_slave(self, msg):\n",
    "        self.queue.put((self.identifier, msg))\n",
    "        ret = self.result.get()\n",
    "        self.queue.put(True)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class SyncMaster(object):\n",
    "    \"\"\"An abstract `SyncMaster` object.\n",
    "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
    "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
    "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
    "    and passed to a registered callback.\n",
    "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
    "    back to each slave devices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_callback):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
    "        \"\"\"\n",
    "        self._master_callback = master_callback\n",
    "        self._queue = queue.Queue()\n",
    "        self._registry = collections.OrderedDict()\n",
    "        self._activated = False\n",
    "\n",
    "    def register_slave(self, identifier):\n",
    "        \"\"\"\n",
    "        Register an slave device.\n",
    "        Args:\n",
    "            identifier: an identifier, usually is the device id.\n",
    "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
    "        \"\"\"\n",
    "        if self._activated:\n",
    "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
    "            self._activated = False\n",
    "            self._registry.clear()\n",
    "        future = FutureResult()\n",
    "        self._registry[identifier] = _MasterRegistry(future)\n",
    "        return SlavePipe(identifier, self._queue, future)\n",
    "\n",
    "    def run_master(self, master_msg):\n",
    "        \"\"\"\n",
    "        Main entry for the master device in each forward pass.\n",
    "        The messages were first collected from each devices (including the master device), and then\n",
    "        an callback will be invoked to compute the message to be sent back to each devices\n",
    "        (including the master device).\n",
    "        Args:\n",
    "            master_msg: the message that the master want to send to itself. This will be placed as the first\n",
    "            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n",
    "        Returns: the message to be sent back to the master device.\n",
    "        \"\"\"\n",
    "        self._activated = True\n",
    "\n",
    "        intermediates = [(0, master_msg)]\n",
    "        for i in range(self.nr_slaves):\n",
    "            intermediates.append(self._queue.get())\n",
    "\n",
    "        results = self._master_callback(intermediates)\n",
    "        assert results[0][0] == 0, 'The first result should belongs to the master.'\n",
    "\n",
    "        for i, res in results:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            self._registry[i].result.put(res)\n",
    "\n",
    "        for i in range(self.nr_slaves):\n",
    "            assert self._queue.get() is True\n",
    "\n",
    "        return results[0][1]\n",
    "\n",
    "    @property\n",
    "    def nr_slaves(self):\n",
    "        return len(self._registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c83c85cbee526168b43c38f629d070e5e0c42088"
   },
   "source": [
    "### 3.7 replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "37ef22cc05218ba282d6433eb7c5b9635c82a97e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# File   : replicate.py\n",
    "# Author : Jiayuan Mao\n",
    "# Email  : maojiayuan@gmail.com\n",
    "# Date   : 27/01/2018\n",
    "#\n",
    "# This file is part of Synchronized-BatchNorm-PyTorch.\n",
    "# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
    "# Distributed under MIT License.\n",
    "\n",
    "import functools\n",
    "\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "\n",
    "__all__ = [\n",
    "    'CallbackContext',\n",
    "    'execute_replication_callbacks',\n",
    "    'DataParallelWithCallback',\n",
    "    'patch_replication_callback'\n",
    "]\n",
    "\n",
    "\n",
    "class CallbackContext(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def execute_replication_callbacks(modules):\n",
    "    \"\"\"\n",
    "    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "    Note that, as all modules are isomorphism, we assign each sub-module with a context\n",
    "    (shared among multiple copies of this module on different devices).\n",
    "    Through this context, different copies can share some information.\n",
    "    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n",
    "    of any slave copies.\n",
    "    \"\"\"\n",
    "    master_copy = modules[0]\n",
    "    nr_modules = len(list(master_copy.modules()))\n",
    "    ctxs = [CallbackContext() for _ in range(nr_modules)]\n",
    "\n",
    "    for i, module in enumerate(modules):\n",
    "        for j, m in enumerate(module.modules()):\n",
    "            if hasattr(m, '__data_parallel_replicate__'):\n",
    "                m.__data_parallel_replicate__(ctxs[j], i)\n",
    "\n",
    "\n",
    "class DataParallelWithCallback(DataParallel):\n",
    "    \"\"\"\n",
    "    Data Parallel with a replication callback.\n",
    "    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n",
    "    original `replicate` function.\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "    Examples:\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "        # sync_bn.__data_parallel_replicate__ will be invoked.\n",
    "    \"\"\"\n",
    "\n",
    "    def replicate(self, module, device_ids):\n",
    "        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n",
    "        execute_replication_callbacks(modules)\n",
    "        return modules\n",
    "\n",
    "\n",
    "def patch_replication_callback(data_parallel):\n",
    "    \"\"\"\n",
    "    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n",
    "    Useful when you have customized `DataParallel` implementation.\n",
    "    Examples:\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n",
    "        > patch_replication_callback(sync_bn)\n",
    "        # this is equivalent to\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(data_parallel, DataParallel)\n",
    "\n",
    "    old_replicate = data_parallel.replicate\n",
    "\n",
    "    @functools.wraps(old_replicate)\n",
    "    def new_replicate(module, device_ids):\n",
    "        modules = old_replicate(module, device_ids)\n",
    "        execute_replication_callbacks(modules)\n",
    "        return modules\n",
    "\n",
    "    data_parallel.replicate = new_replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6c0c1a82bba2c269d9c8a996fd33821b562cae8"
   },
   "source": [
    "### 3.8 batchnorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "08e9977b4d8b5cdaced13042723e942e91d207b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/syncbn.py\n",
    "## https://github.com/zhanghang1989/PyTorch-Encoding\n",
    "## https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# File   : batchnorm.py\n",
    "# Author : Jiayuan Mao\n",
    "# Email  : maojiayuan@gmail.com\n",
    "# Date   : 27/01/2018\n",
    "#\n",
    "# This file is part of Synchronized-BatchNorm-PyTorch.\n",
    "# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
    "# Distributed under MIT License.\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
    "\n",
    "\n",
    "__all__ = ['SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d', 'SynchronizedBatchNorm3d']\n",
    "\n",
    "\n",
    "def _sum_ft(tensor):\n",
    "    \"\"\"sum over the first and last dimention\"\"\"\n",
    "    return tensor.sum(dim=0).sum(dim=-1)\n",
    "\n",
    "\n",
    "def _unsqueeze_ft(tensor):\n",
    "    \"\"\"add new dementions at the front and the tail\"\"\"\n",
    "    return tensor.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "\n",
    "_ChildMessage = collections.namedtuple('_ChildMessage', ['sum', 'ssum', 'sum_size'])\n",
    "_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n",
    "\n",
    "\n",
    "class _SynchronizedBatchNorm(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
    "        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n",
    "\n",
    "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
    "\n",
    "        self._is_parallel = False\n",
    "        self._parallel_id = None\n",
    "        self._slave_pipe = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
    "        if not (self._is_parallel and self.training):\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                self.training, self.momentum, self.eps)\n",
    "\n",
    "        # Resize the input to (B, C, -1).\n",
    "        input_shape = input.size()\n",
    "        input = input.view(input.size(0), self.num_features, -1)\n",
    "\n",
    "        # Compute the sum and square-sum.\n",
    "        sum_size = input.size(0) * input.size(2)\n",
    "        input_sum = _sum_ft(input)\n",
    "        input_ssum = _sum_ft(input ** 2)\n",
    "\n",
    "        # Reduce-and-broadcast the statistics.\n",
    "        if self._parallel_id == 0:\n",
    "            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "        else:\n",
    "            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "\n",
    "        # Compute the output.\n",
    "        if self.affine:\n",
    "            # MJY:: Fuse the multiplication for speed.\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
    "        else:\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
    "\n",
    "        # Reshape it.\n",
    "        return output.view(input_shape)\n",
    "\n",
    "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
    "        self._is_parallel = True\n",
    "        self._parallel_id = copy_id\n",
    "\n",
    "        # parallel_id == 0 means master device.\n",
    "        if self._parallel_id == 0:\n",
    "            ctx.sync_master = self._sync_master\n",
    "        else:\n",
    "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
    "\n",
    "    def _data_parallel_master(self, intermediates):\n",
    "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
    "\n",
    "        # Always using same \"device order\" makes the ReduceAdd operation faster.\n",
    "        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n",
    "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
    "\n",
    "        to_reduce = [i[1][:2] for i in intermediates]\n",
    "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
    "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
    "\n",
    "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
    "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
    "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
    "\n",
    "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
    "\n",
    "        outputs = []\n",
    "        for i, rec in enumerate(intermediates):\n",
    "            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _compute_mean_std(self, sum_, ssum, size):\n",
    "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
    "        also maintains the moving average on the master device.\"\"\"\n",
    "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
    "        mean = sum_ / size\n",
    "        sumvar = ssum - sum_ * mean\n",
    "        unbias_var = sumvar / (size - 1)\n",
    "        bias_var = sumvar / size\n",
    "\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n",
    "\n",
    "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n",
    "    mini-batch.\n",
    "    .. math::\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of size\n",
    "            `batch_size x num_features [x width]`\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
    "        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm1d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 2 and input.dim() != 3:\n",
    "            raise ValueError('expected 2D or 3D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
    "    of 3d inputs\n",
    "    .. math::\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 5d input that is seen as a mini-batch\n",
    "    of 4d inputs\n",
    "    .. math::\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric BatchNorm\n",
    "    or Spatio-temporal BatchNorm\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x depth x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, D, H, W)`\n",
    "        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm3d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 5:\n",
    "            raise ValueError('expected 5D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a514ffd32da3b95a5e8639384603667a9136d163"
   },
   "source": [
    "### 3.9 transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "db96679dd815cbd3c5bbdaf48a00cb31a0adf99b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "def do_resize(image, H, W):\n",
    "    image = cv2.resize(image, dsize=(W, H))\n",
    "    return image\n",
    "\n",
    "\n",
    "def do_resize2(image, mask, H, W):\n",
    "    image = cv2.resize(image, dsize=(W, H))\n",
    "    mask = cv2.resize(mask, dsize=(W, H))\n",
    "    mask = (mask > 0.5).astype(np.float32)\n",
    "\n",
    "    return image, mask\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def compute_center_pad(H, W, factor=32):\n",
    "\n",
    "    if H % factor == 0:\n",
    "        dy0, dy1 = 0, 0\n",
    "    else:\n",
    "        dy = factor - H % factor\n",
    "        dy0 = dy//2\n",
    "        dy1 = dy - dy0\n",
    "\n",
    "    if W % factor == 0:\n",
    "        dx0, dx1 = 0, 0\n",
    "    else:\n",
    "        dx = factor - W % factor\n",
    "        dx0 = dx//2\n",
    "        dx1 = dx - dx0\n",
    "\n",
    "    return dy0, dy1, dx0, dx1\n",
    "\n",
    "\n",
    "def do_center_pad_to_factor(image, factor=32):\n",
    "    H, W = image.shape[:2]\n",
    "    dy0, dy1, dx0, dx1 = compute_center_pad(H, W, factor)\n",
    "\n",
    "    image = cv2.copyMakeBorder(\n",
    "        image, dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n",
    "    # cv2.BORDER_CONSTANT, 0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def do_center_pad_to_factor2(image, mask, factor=32):\n",
    "    image = do_center_pad_to_factor(image, factor)\n",
    "    mask = do_center_pad_to_factor(mask, factor)\n",
    "    return image, mask\n",
    "\n",
    "def do_center_pad2(image, mask, PAD):\n",
    "    image = cv2.copyMakeBorder(\n",
    "        image, PAD, PAD, PAD, PAD, cv2.BORDER_REFLECT_101)\n",
    "    mask = cv2.copyMakeBorder(\n",
    "        mask, PAD, PAD, PAD, PAD, cv2.BORDER_REFLECT_101)\n",
    "    return image, mask \n",
    "\n",
    "def do_center_pad(image, PAD):\n",
    "    image = cv2.copyMakeBorder(\n",
    "        image, PAD, PAD, PAD, PAD, cv2.BORDER_REFLECT_101)\n",
    "    return image \n",
    "#---\n",
    "\n",
    "\n",
    "def do_horizontal_flip(image):\n",
    "    # flip left-right\n",
    "    image = cv2.flip(image, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def do_horizontal_flip2(image, mask):\n",
    "    image = do_horizontal_flip(image)\n",
    "    mask = do_horizontal_flip(mask)\n",
    "    return image, mask\n",
    "\n",
    "#---\n",
    "\n",
    "\n",
    "def compute_random_pad(H, W, limit=(-4, 4), factor=32):\n",
    "\n",
    "    if H % factor == 0:\n",
    "        dy0, dy1 = 0, 0\n",
    "    else:\n",
    "        dy = factor - H % factor\n",
    "        # np.random.choice(dy)\n",
    "        dy0 = dy//2 + np.random.randint(limit[0], limit[1])\n",
    "        dy1 = dy - dy0\n",
    "\n",
    "    if W % factor == 0:\n",
    "        dx0, dx1 = 0, 0\n",
    "    else:\n",
    "        dx = factor - W % factor\n",
    "        # np.random.choice(dx)\n",
    "        dx0 = dx//2 + np.random.randint(limit[0], limit[1])\n",
    "        dx1 = dx - dx0\n",
    "\n",
    "    return dy0, dy1, dx0, dx1\n",
    "\n",
    "\n",
    "def do_random_pad_to_factor2(image, mask, limit=(-4, 4), factor=32):\n",
    "    H, W = image.shape[:2]\n",
    "    dy0, dy1, dx0, dx1 = compute_random_pad(H, W, limit, factor)\n",
    "\n",
    "    image = cv2.copyMakeBorder(\n",
    "        image, dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n",
    "    mask = cv2.copyMakeBorder(mask,  dy0, dy1, dx0,\n",
    "                              dx1, cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "#----\n",
    "\n",
    "\n",
    "def do_invert_intensity(image):\n",
    "    # flip left-right\n",
    "    image = np.clip(1-image, 0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def do_brightness_shift(image, alpha=0.125):\n",
    "    image = image + alpha\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def do_brightness_multiply(image, alpha=1):\n",
    "    image = alpha*image\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "# https://www.pyimagesearch.com/2015/10/05/opencv-gamma-correction/\n",
    "def do_gamma(image, gamma=1.0):\n",
    "\n",
    "    image = image ** (1.0 / gamma)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def do_flip_transpose2(image, mask, type=0):\n",
    "    # choose one of the 8 cases\n",
    "\n",
    "    if type == 1:  # rotate90\n",
    "        image = image.transpose(1, 0)\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        mask = mask.transpose(1, 0)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "\n",
    "    if type == 2:  # rotate180\n",
    "        image = cv2.flip(image, -1)\n",
    "        mask = cv2.flip(mask, -1)\n",
    "\n",
    "    if type == 3:  # rotate270\n",
    "        image = image.transpose(1, 0)\n",
    "        image = cv2.flip(image, 0)\n",
    "\n",
    "        mask = mask.transpose(1, 0)\n",
    "        mask = cv2.flip(mask, 0)\n",
    "\n",
    "    if type == 4:  # flip left-right\n",
    "        image = cv2.flip(image, 1)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "\n",
    "    if type == 5:  # flip up-down\n",
    "        image = cv2.flip(image, 0)\n",
    "        mask = cv2.flip(mask, 0)\n",
    "\n",
    "    if type == 6:\n",
    "        image = cv2.flip(image, 1)\n",
    "        image = image.transpose(1, 0)\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        mask = cv2.flip(mask, 1)\n",
    "        mask = mask.transpose(1, 0)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "\n",
    "    if type == 7:\n",
    "        image = cv2.flip(image, 0)\n",
    "        image = image.transpose(1, 0)\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        mask = cv2.flip(mask, 0)\n",
    "        mask = mask.transpose(1, 0)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "# ================================\n",
    "\n",
    "\n",
    "def do_shift_scale_crop(image, mask, x0=0, y0=0, x1=1, y1=1):\n",
    "    # cv2.BORDER_REFLECT_101\n",
    "    # cv2.BORDER_CONSTANT\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    image = image[y0:y1, x0:x1]\n",
    "    mask = mask[y0:y1, x0:x1]\n",
    "\n",
    "    image = cv2.resize(image, dsize=(width, height))\n",
    "    mask = cv2.resize(mask, dsize=(width, height))\n",
    "    mask = (mask > 0.5).astype(np.float32)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def do_random_shift_scale_crop_pad2(image, mask, limit=0.10):\n",
    "\n",
    "    H, W = image.shape[:2]\n",
    "\n",
    "    dy = int(H*limit)\n",
    "    y0 = np.random.randint(0, dy)\n",
    "    y1 = H-np.random.randint(0, dy)\n",
    "\n",
    "    dx = int(W*limit)\n",
    "    x0 = np.random.randint(0, dx)\n",
    "    x1 = W-np.random.randint(0, dx)\n",
    "\n",
    "    #y0, y1, x0, x1\n",
    "    image, mask = do_shift_scale_crop(image, mask, x0, y0, x1, y1)\n",
    "    return image, mask\n",
    "\n",
    "#===========================================================================\n",
    "\n",
    "\n",
    "def do_shift_scale_rotate2(image, mask, dx=0, dy=0, scale=1, angle=0):\n",
    "    borderMode = cv2.BORDER_REFLECT_101\n",
    "    # cv2.BORDER_REFLECT_101  cv2.BORDER_CONSTANT\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    sx = scale\n",
    "    sy = scale\n",
    "    cc = math.cos(angle/180*math.pi)*(sx)\n",
    "    ss = math.sin(angle/180*math.pi)*(sy)\n",
    "    rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n",
    "\n",
    "    box0 = np.array([[0, 0], [width, 0],  [width, height],\n",
    "                     [0, height], ], np.float32)\n",
    "    box1 = box0 - np.array([width/2, height/2])\n",
    "    box1 = np.dot(box1, rotate_matrix.T) + np.array([width/2+dx, height/2+dy])\n",
    "\n",
    "    box0 = box0.astype(np.float32)\n",
    "    box1 = box1.astype(np.float32)\n",
    "    mat = cv2.getPerspectiveTransform(box0, box1)\n",
    "\n",
    "    image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                                borderMode=borderMode, borderValue=(0, 0, 0,))  # cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n",
    "    mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_NEAREST,  # cv2.INTER_LINEAR\n",
    "                               borderMode=borderMode, borderValue=(0, 0, 0,))  # cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n",
    "    mask = (mask > 0.5).astype(np.float32)\n",
    "    return image, mask\n",
    "\n",
    "# https://www.kaggle.com/ori226/data-augmentation-with-elastic-deformations\n",
    "# https://github.com/letmaik/lensfunpy/blob/master/lensfunpy/util.py\n",
    "\n",
    "\n",
    "def do_elastic_transform2(image, mask, grid=32, distort=0.2):\n",
    "    borderMode = cv2.BORDER_REFLECT_101\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    x_step = int(grid)\n",
    "    xx = np.zeros(width, np.float32)\n",
    "    prev = 0\n",
    "    for x in range(0, width, x_step):\n",
    "        start = x\n",
    "        end = x + x_step\n",
    "        if end > width:\n",
    "            end = width\n",
    "            cur = width\n",
    "        else:\n",
    "            cur = prev + x_step*(1+random.uniform(-distort, distort))\n",
    "\n",
    "        xx[start:end] = np.linspace(prev, cur, end-start)\n",
    "        prev = cur\n",
    "\n",
    "    y_step = int(grid)\n",
    "    yy = np.zeros(height, np.float32)\n",
    "    prev = 0\n",
    "    for y in range(0, height, y_step):\n",
    "        start = y\n",
    "        end = y + y_step\n",
    "        if end > height:\n",
    "            end = height\n",
    "            cur = height\n",
    "        else:\n",
    "            cur = prev + y_step*(1+random.uniform(-distort, distort))\n",
    "\n",
    "        yy[start:end] = np.linspace(prev, cur, end-start)\n",
    "        prev = cur\n",
    "\n",
    "    # grid\n",
    "    map_x, map_y = np.meshgrid(xx, yy)\n",
    "    map_x = map_x.astype(np.float32)\n",
    "    map_y = map_y.astype(np.float32)\n",
    "\n",
    "    #image = map_coordinates(image, coords, order=1, mode='reflect').reshape(shape)\n",
    "    image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR,\n",
    "                      borderMode=borderMode, borderValue=(0, 0, 0,))\n",
    "\n",
    "    mask = cv2.remap(mask, map_x, map_y, interpolation=cv2.INTER_NEAREST,\n",
    "                     borderMode=borderMode, borderValue=(0, 0, 0,))\n",
    "    mask = (mask > 0.5).astype(np.float32)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def do_horizontal_shear2(image, mask, dx=0):\n",
    "    borderMode = cv2.BORDER_REFLECT_101\n",
    "    # cv2.BORDER_REFLECT_101  cv2.BORDER_CONSTANT\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    dx = int(dx*width)\n",
    "\n",
    "    box0 = np.array([[0, 0], [width, 0],  [width, height],\n",
    "                     [0, height], ], np.float32)\n",
    "    box1 = np.array([[+dx, 0], [width+dx, 0],  [width-dx,\n",
    "                                                height], [-dx, height], ], np.float32)\n",
    "\n",
    "    box0 = box0.astype(np.float32)\n",
    "    box1 = box1.astype(np.float32)\n",
    "    mat = cv2.getPerspectiveTransform(box0, box1)\n",
    "\n",
    "    image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                                borderMode=borderMode, borderValue=(0, 0, 0,))  # cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n",
    "    mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_NEAREST,  # cv2.INTER_LINEAR\n",
    "                               borderMode=borderMode, borderValue=(0, 0, 0,))  # cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n",
    "    mask = (mask > 0.5).astype(np.float32)\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c12b94eead1ce741ba13ac70e54fcaf7e04a65dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:221: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test_df, train_df = GenTrainTest_df_cv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "c9072529ea0c160dc5dd5e85832283f9ad46da41"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z</th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "      <th>coverage</th>\n",
       "      <th>coverage_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>575d24d81d</th>\n",
       "      <td>843</td>\n",
       "      <td>[[0.5254902, 0.5137255, 0.5254902, 0.5372549, ...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a266a2a9df</th>\n",
       "      <td>794</td>\n",
       "      <td>[[0.34117648, 0.3764706, 0.33333334, 0.2235294...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.504950</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75efad62c1</th>\n",
       "      <td>468</td>\n",
       "      <td>[[0.5686275, 0.46666667, 0.3254902, 0.2627451,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.993334</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34e51dba6a</th>\n",
       "      <td>727</td>\n",
       "      <td>[[0.5411765, 0.4745098, 0.39607844, 0.30588236...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.149201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875705fb0</th>\n",
       "      <td>797</td>\n",
       "      <td>[[0.06666667, 0.078431375, 0.09019608, 0.10588...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.042839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              z                                             images  \\\n",
       "id                                                                   \n",
       "575d24d81d  843  [[0.5254902, 0.5137255, 0.5254902, 0.5372549, ...   \n",
       "a266a2a9df  794  [[0.34117648, 0.3764706, 0.33333334, 0.2235294...   \n",
       "75efad62c1  468  [[0.5686275, 0.46666667, 0.3254902, 0.2627451,...   \n",
       "34e51dba6a  727  [[0.5411765, 0.4745098, 0.39607844, 0.30588236...   \n",
       "4875705fb0  797  [[0.06666667, 0.078431375, 0.09019608, 0.10588...   \n",
       "\n",
       "                                                        masks  coverage  \\\n",
       "id                                                                        \n",
       "575d24d81d  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.000000   \n",
       "a266a2a9df  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.504950   \n",
       "75efad62c1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.993334   \n",
       "34e51dba6a  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.149201   \n",
       "4875705fb0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.042839   \n",
       "\n",
       "            coverage_class  \n",
       "id                          \n",
       "575d24d81d               0  \n",
       "a266a2a9df               6  \n",
       "75efad62c1              10  \n",
       "34e51dba6a               2  \n",
       "4875705fb0               1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "087a574fc07541d1cb2214fb6fe8eb73e2a5fd27"
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "368b0affe9518ac4b433f08f05e35187a1c0d49b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "5e8e578f43626c9cffc17a8f767ac140b57ff7b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val(model, dataloader):\n",
    "    \"\"\"\n",
    "    计算模型在验证集上的准确率等信息\n",
    "    \"\"\"\n",
    "    #val_loss = torch.nn.BCELoss()\n",
    "    #val_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    #val_loss = mixed_dice_bce_loss(opt.dice_weight, opt.bce_weight)\n",
    "    print('fold validating...')\n",
    "    val_loss = RobustFocalLoss2d()\n",
    "    model.eval()\n",
    "    vallosslist = []\n",
    "    meanioulist = []\n",
    "    val_predictions = []\n",
    "    val_masks = []\n",
    "    for ii, data in tqdm(enumerate(dataloader)):\n",
    "        input, label = data\n",
    "        val_input = Variable(input, volatile=True)\n",
    "        val_target = Variable(label, volatile=True)\n",
    "        if opt.use_gpu:\n",
    "            val_input = val_input.cuda()\n",
    "            val_target = val_target.cuda()\n",
    "        score = model(val_input)\n",
    "        val_predictions.append(torch.squeeze(F.sigmoid(score)).cpu().data.numpy())\n",
    "        val_masks.append(val_target.cpu().data.numpy())\n",
    "        valloss = val_loss(score, val_target)\n",
    "        vallosslist.append(valloss.cpu().data[0])\n",
    "    val_predictions = np.vstack(val_predictions)\n",
    "    val_masks = np.vstack(val_masks)\n",
    "    bestThred, bestIOU = GetBestThred(val_masks, val_predictions)\n",
    "    model.train()\n",
    "    return np.mean(vallosslist), bestIOU, bestThred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7ba6151fd337c276cb29f5b5eaeea2898cb90bd5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, bestThred, test_dataloader):\n",
    "    if opt.use_gpu:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    print('fold testing...')\n",
    "    test_df['pred_fold%d' %n_fold] = np.nan\n",
    "    test_df['pred_fold%d' %n_fold] = test_df['pred_fold%d' %n_fold].astype(object)\n",
    "    for ii, (data, img_ids) in tqdm(enumerate(test_dataloader)):\n",
    "        input = torch.autograd.Variable(data, volatile=True)\n",
    "        if opt.use_gpu:\n",
    "            input = input.cuda()\n",
    "        score = torch.squeeze(F.sigmoid(model(input)))\n",
    "        test_df.loc[list(img_ids), 'pred_fold%d' %\n",
    "                    n_fold] = list(score.cpu().data.numpy())\n",
    "    test_df['thred_pred_fold%d_%f' % (n_fold,bestThred)] = test_df['pred_fold%d' % n_fold].apply(lambda x:(x > bestThred).astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "894339b765553aa266536074c4f2738932c973e0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(n_fold, train_idx, valid_idx):\n",
    "    # step1: configure model\n",
    "    BestModel = None\n",
    "    BestThred = opt.BestThred\n",
    "    BestIou = -1\n",
    "    print('loading model...')\n",
    "    model = UNetResNet34()\n",
    "    print('successfully loading model...')\n",
    "    if opt.load_model_path:\n",
    "        model.load(opt.load_model_path)\n",
    "    if opt.use_gpu:\n",
    "        model.cuda()\n",
    "    previous_loss = 1e100\n",
    "    lr = opt.lr\n",
    "    print('Fold:%d...' % n_fold)\n",
    "    train_data = TrainDataSet(train_df.iloc[train_idx].index.tolist(), opt)\n",
    "    val_data = ValDataSet(train_df.iloc[valid_idx].index.tolist(), opt)\n",
    "    train_dataloader = DataLoader(train_data, opt.batch_size,\n",
    "                                    shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, opt.batch_size,\n",
    "                                    shuffle=False)\n",
    "    #criterion = torch.nn.BCELoss()\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    #criterion = mixed_dice_bce_loss(opt.dice_weight, opt.bce_weight)\n",
    "    criterion = RobustFocalLoss2d()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=opt.weight_decay)\n",
    "    #optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    # train\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_iou = []\n",
    "    val_bestthred_list = []\n",
    "    for epoch in range(opt.max_epoch):\n",
    "        print('Epoch:%d...' % epoch)\n",
    "        train_loss_batch = []\n",
    "        for ii, (data, label) in tqdm(enumerate(train_dataloader)):\n",
    "            # train model\n",
    "            input = Variable(data)\n",
    "            target = Variable(label)\n",
    "            if opt.use_gpu:\n",
    "                model = model.cuda()\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            score = model(input)\n",
    "            loss = criterion(score, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ii % opt.print_freq == opt.print_freq - 1:\n",
    "                print('Fold:%d\\t Epoch:%d\\t Batch:%d\\t loss:%f' %\n",
    "                        (n_fold, epoch, ii, loss.data[0]))\n",
    "            train_loss_batch.append(loss.data[0])\n",
    "            # validate and visualize\n",
    "        val_meanloss, val_meaniou, val_bestthred = val(model, val_dataloader)\n",
    "        print('Fold:%d\\t Epoch:%d\\t meanTrainLoss:%f\\t meanValLoss:%f\\t meanValIOU:%f\\t BestValThred:%f' %\n",
    "                (n_fold, epoch, np.mean(train_loss_batch), val_meanloss, val_meaniou, val_bestthred))\n",
    "        train_loss.append(np.mean(train_loss_batch))\n",
    "        val_loss.append(val_meanloss)\n",
    "        val_iou.append(val_meaniou)\n",
    "        val_bestthred_list.append(val_bestthred)\n",
    "        # update learning rate\n",
    "        if val_meanloss > previous_loss:\n",
    "            lr = lr * opt.lr_decay\n",
    "            # 第二种降低学习率的方法:不会有moment等信息的丢失\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        previous_loss = val_meanloss\n",
    "        #save the best model\n",
    "        if val_meaniou>BestIou:\n",
    "            BestIou = val_meaniou\n",
    "            BestModel = model\n",
    "            BestThred = val_bestthred\n",
    "    return BestModel,BestIou,BestThred,train_loss,val_loss,val_iou,val_bestthred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3acaa0bbda5bf3143e4272a8e7fa1aa6af94123f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_result(train_loss,val_loss,val_iou,val_bestthred):\n",
    "    plt.figure(1)\n",
    "    plt.title('train loss vs val loss')\n",
    "    plt.plot(train_loss,label='train loss')\n",
    "    plt.plot(val_loss,label='train loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.figure(2)\n",
    "    plt.title('val mean iou')\n",
    "    plt.plot(val_iou)\n",
    "    plt.ylabel('mean iou')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.figure(3)\n",
    "    plt.title('best thred')\n",
    "    plt.plot(val_bestthred)\n",
    "    plt.ylabel('val_bestthred')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "461ccf4aca7b3d468013140003aae465c0ebd538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------>StratifiedKFold:5...\n"
     ]
    }
   ],
   "source": [
    "if opt.stratified:\n",
    "    print('------>StratifiedKFold:%d...' % opt.num_folds)\n",
    "    folds = StratifiedKFold(\n",
    "        n_splits=opt.num_folds, shuffle=True, random_state=opt.seed)\n",
    "else:\n",
    "    print('------>KFold%d...' % opt.num_folds)\n",
    "    folds = KFold(n_splits=opt.num_folds,\n",
    "                    shuffle=True, random_state=opt.seed)\n",
    "fold_ = folds.split(train_df['images'], train_df['coverage_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "52d919cbf17eefb29b01a2e9956be5e26495f8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /tmp/.torch/models/resnet34-333f7ec4.pth\n",
      "100%|██████████| 87306240/87306240 [00:02<00:00, 38672636.32it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loading model...\n",
      "Fold:0...\n",
      "Epoch:0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:0\t Batch:19\t loss:0.089314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:0\t Batch:39\t loss:0.141314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:34,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:0\t Batch:59\t loss:0.107082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:05,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:0\t Batch:79\t loss:0.127790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:36,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:0\t Batch:99\t loss:0.108303\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.20it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:0\t meanTrainLoss:0.119524\t meanValLoss:0.146957\t meanValIOU:0.441791\t BestValThred:0.500000\n",
      "Epoch:1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:1\t Batch:19\t loss:0.066590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:1\t Batch:39\t loss:0.081300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:1\t Batch:59\t loss:0.086146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:1\t Batch:79\t loss:0.111380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:1\t Batch:99\t loss:0.139304\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.20it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:1\t meanTrainLoss:0.102716\t meanValLoss:0.160804\t meanValIOU:0.408458\t BestValThred:0.500000\n",
      "Epoch:2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:2\t Batch:19\t loss:0.120159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:2\t Batch:39\t loss:0.102611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:2\t Batch:59\t loss:0.124345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:2\t Batch:79\t loss:0.091955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:2\t Batch:99\t loss:0.092659\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.22it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:2\t meanTrainLoss:0.098600\t meanValLoss:0.123705\t meanValIOU:0.462687\t BestValThred:0.450000\n",
      "Epoch:3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:3\t Batch:19\t loss:0.092532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:3\t Batch:39\t loss:0.079371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:3\t Batch:59\t loss:0.069954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:3\t Batch:79\t loss:0.098896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:3\t Batch:99\t loss:0.120535\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.22it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:3\t meanTrainLoss:0.093593\t meanValLoss:0.118724\t meanValIOU:0.489303\t BestValThred:0.400000\n",
      "Epoch:4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:4\t Batch:19\t loss:0.083969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:4\t Batch:39\t loss:0.088373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:4\t Batch:59\t loss:0.079608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:4\t Batch:79\t loss:0.061717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:4\t Batch:99\t loss:0.119181\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.22it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:4\t meanTrainLoss:0.090078\t meanValLoss:0.123544\t meanValIOU:0.415796\t BestValThred:0.450000\n",
      "Epoch:5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:5\t Batch:19\t loss:0.065250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:5\t Batch:39\t loss:0.062819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:5\t Batch:59\t loss:0.074951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:5\t Batch:79\t loss:0.071441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:5\t Batch:99\t loss:0.088727\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:5\t meanTrainLoss:0.087563\t meanValLoss:0.231139\t meanValIOU:0.443408\t BestValThred:0.650000\n",
      "Epoch:6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:6\t Batch:19\t loss:0.102824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:6\t Batch:39\t loss:0.095176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:6\t Batch:59\t loss:0.063273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:6\t Batch:79\t loss:0.070796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:6\t Batch:99\t loss:0.081430\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:6\t meanTrainLoss:0.085634\t meanValLoss:0.084414\t meanValIOU:0.551617\t BestValThred:0.450000\n",
      "Epoch:7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:7\t Batch:19\t loss:0.082875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:7\t Batch:39\t loss:0.067094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:7\t Batch:59\t loss:0.076523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:7\t Batch:79\t loss:0.078600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:7\t Batch:99\t loss:0.087048\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.22it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:7\t meanTrainLoss:0.076469\t meanValLoss:0.076937\t meanValIOU:0.577114\t BestValThred:0.450000\n",
      "Epoch:8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:8\t Batch:19\t loss:0.086518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:8\t Batch:39\t loss:0.075985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:8\t Batch:59\t loss:0.054627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:8\t Batch:79\t loss:0.120752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:8\t Batch:99\t loss:0.065768\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:8\t meanTrainLoss:0.075825\t meanValLoss:0.068509\t meanValIOU:0.613433\t BestValThred:0.550000\n",
      "Epoch:9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:9\t Batch:19\t loss:0.072930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:9\t Batch:39\t loss:0.068376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:9\t Batch:59\t loss:0.077763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:9\t Batch:79\t loss:0.055559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:9\t Batch:99\t loss:0.095440\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:9\t meanTrainLoss:0.074803\t meanValLoss:0.069328\t meanValIOU:0.552114\t BestValThred:0.550000\n",
      "Epoch:10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:10\t Batch:19\t loss:0.052256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:10\t Batch:39\t loss:0.066045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:10\t Batch:59\t loss:0.106558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:10\t Batch:79\t loss:0.053056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:10\t Batch:99\t loss:0.077193\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:10\t meanTrainLoss:0.069740\t meanValLoss:0.074331\t meanValIOU:0.579602\t BestValThred:0.450000\n",
      "Epoch:11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:11\t Batch:19\t loss:0.078223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:11\t Batch:39\t loss:0.059776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:11\t Batch:59\t loss:0.054538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:11\t Batch:79\t loss:0.047594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:11\t Batch:99\t loss:0.064392\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:11\t meanTrainLoss:0.065395\t meanValLoss:0.067661\t meanValIOU:0.600746\t BestValThred:0.550000\n",
      "Epoch:12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:12\t Batch:19\t loss:0.045640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:12\t Batch:39\t loss:0.087132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:12\t Batch:59\t loss:0.087014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:12\t Batch:79\t loss:0.048820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:12\t Batch:99\t loss:0.053783\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:12\t meanTrainLoss:0.068881\t meanValLoss:0.085213\t meanValIOU:0.565920\t BestValThred:0.450000\n",
      "Epoch:13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:13\t Batch:19\t loss:0.069467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:13\t Batch:39\t loss:0.064655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:13\t Batch:59\t loss:0.050488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:13\t Batch:79\t loss:0.094251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:13\t Batch:99\t loss:0.163432\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:13\t meanTrainLoss:0.064228\t meanValLoss:0.089657\t meanValIOU:0.531095\t BestValThred:0.700000\n",
      "Epoch:14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:14\t Batch:19\t loss:0.075814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:14\t Batch:39\t loss:0.074597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:14\t Batch:59\t loss:0.054126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:14\t Batch:79\t loss:0.041886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:14\t Batch:99\t loss:0.047608\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:14\t meanTrainLoss:0.062090\t meanValLoss:0.073223\t meanValIOU:0.613060\t BestValThred:0.500000\n",
      "Epoch:15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:15\t Batch:19\t loss:0.041434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:15\t Batch:39\t loss:0.039781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:15\t Batch:59\t loss:0.045729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:15\t Batch:79\t loss:0.074654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:15\t Batch:99\t loss:0.079212\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:15\t meanTrainLoss:0.059500\t meanValLoss:0.067702\t meanValIOU:0.611816\t BestValThred:0.650000\n",
      "Epoch:16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:16\t Batch:19\t loss:0.056538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:16\t Batch:39\t loss:0.063393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:16\t Batch:59\t loss:0.040641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:16\t Batch:79\t loss:0.064003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:16\t Batch:99\t loss:0.048117\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.22it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:16\t meanTrainLoss:0.056651\t meanValLoss:0.049430\t meanValIOU:0.706095\t BestValThred:0.500000\n",
      "Epoch:17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:17\t Batch:19\t loss:0.109954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:17\t Batch:39\t loss:0.044130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:17\t Batch:59\t loss:0.047643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:17\t Batch:79\t loss:0.060596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:17\t Batch:99\t loss:0.046119\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:17\t meanTrainLoss:0.058305\t meanValLoss:0.047301\t meanValIOU:0.696766\t BestValThred:0.550000\n",
      "Epoch:18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:18\t Batch:19\t loss:0.052960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:18\t Batch:39\t loss:0.033717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:18\t Batch:59\t loss:0.059502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:18\t Batch:79\t loss:0.059090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:18\t Batch:99\t loss:0.062566\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:18\t meanTrainLoss:0.058613\t meanValLoss:0.057778\t meanValIOU:0.651493\t BestValThred:0.600000\n",
      "Epoch:19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:19\t Batch:19\t loss:0.058581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:19\t Batch:39\t loss:0.053674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:19\t Batch:59\t loss:0.066509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:19\t Batch:79\t loss:0.027601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:19\t Batch:99\t loss:0.060672\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:19\t meanTrainLoss:0.054586\t meanValLoss:0.062397\t meanValIOU:0.635075\t BestValThred:0.500000\n",
      "Epoch:20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:20\t Batch:19\t loss:0.073098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:20\t Batch:39\t loss:0.047016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:20\t Batch:59\t loss:0.065084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:20\t Batch:79\t loss:0.031711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:20\t Batch:99\t loss:0.075204\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:20\t meanTrainLoss:0.053519\t meanValLoss:0.049462\t meanValIOU:0.684328\t BestValThred:0.500000\n",
      "Epoch:21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:21\t Batch:19\t loss:0.041405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:21\t Batch:39\t loss:0.043604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:21\t Batch:59\t loss:0.066264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:21\t Batch:79\t loss:0.062819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:21\t Batch:99\t loss:0.033989\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:21\t meanTrainLoss:0.052625\t meanValLoss:0.046285\t meanValIOU:0.689303\t BestValThred:0.600000\n",
      "Epoch:22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:22\t Batch:19\t loss:0.074080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:22\t Batch:39\t loss:0.037442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:22\t Batch:59\t loss:0.050556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:22\t Batch:79\t loss:0.047816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:22\t Batch:99\t loss:0.032823\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:22\t meanTrainLoss:0.050345\t meanValLoss:0.047361\t meanValIOU:0.694279\t BestValThred:0.550000\n",
      "Epoch:23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:23\t Batch:19\t loss:0.036553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:23\t Batch:39\t loss:0.038986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:23\t Batch:59\t loss:0.056054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:23\t Batch:79\t loss:0.043586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:23\t Batch:99\t loss:0.096556\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:23\t meanTrainLoss:0.048390\t meanValLoss:0.057977\t meanValIOU:0.645025\t BestValThred:0.500000\n",
      "Epoch:24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:24\t Batch:19\t loss:0.044063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:24\t Batch:39\t loss:0.051468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:24\t Batch:59\t loss:0.040648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:24\t Batch:79\t loss:0.050269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:24\t Batch:99\t loss:0.048176\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:24\t meanTrainLoss:0.048042\t meanValLoss:0.047551\t meanValIOU:0.714552\t BestValThred:0.600000\n",
      "Epoch:25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:25\t Batch:19\t loss:0.054309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:25\t Batch:39\t loss:0.045621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:25\t Batch:59\t loss:0.038584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:25\t Batch:79\t loss:0.044073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:25\t Batch:99\t loss:0.035465\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:25\t meanTrainLoss:0.047888\t meanValLoss:0.049441\t meanValIOU:0.704478\t BestValThred:0.500000\n",
      "Epoch:26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:26\t Batch:19\t loss:0.040173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:26\t Batch:39\t loss:0.035745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:26\t Batch:59\t loss:0.033850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:26\t Batch:79\t loss:0.073575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:26\t Batch:99\t loss:0.068866\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:26\t meanTrainLoss:0.048923\t meanValLoss:0.045929\t meanValIOU:0.721891\t BestValThred:0.500000\n",
      "Epoch:27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:27\t Batch:19\t loss:0.040637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:27\t Batch:39\t loss:0.045837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:27\t Batch:59\t loss:0.044262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:27\t Batch:79\t loss:0.042813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:27\t Batch:99\t loss:0.054727\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:27\t meanTrainLoss:0.046036\t meanValLoss:0.054667\t meanValIOU:0.662065\t BestValThred:0.500000\n",
      "Epoch:28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:28\t Batch:19\t loss:0.028422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:28\t Batch:39\t loss:0.020363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:28\t Batch:59\t loss:0.028374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:28\t Batch:79\t loss:0.054822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:28\t Batch:99\t loss:0.035377\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:28\t meanTrainLoss:0.043873\t meanValLoss:0.040829\t meanValIOU:0.724005\t BestValThred:0.500000\n",
      "Epoch:29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:29\t Batch:19\t loss:0.075826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:29\t Batch:39\t loss:0.036972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:29\t Batch:59\t loss:0.022398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:29\t Batch:79\t loss:0.046329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:29\t Batch:99\t loss:0.029533\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:29\t meanTrainLoss:0.044421\t meanValLoss:0.051853\t meanValIOU:0.676368\t BestValThred:0.600000\n",
      "Epoch:30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:30\t Batch:19\t loss:0.070587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:30\t Batch:39\t loss:0.032009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:30\t Batch:59\t loss:0.037760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:30\t Batch:79\t loss:0.058962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:30\t Batch:99\t loss:0.091323\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:30\t meanTrainLoss:0.041684\t meanValLoss:0.042890\t meanValIOU:0.737935\t BestValThred:0.550000\n",
      "Epoch:31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:31\t Batch:19\t loss:0.031594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:31\t Batch:39\t loss:0.043447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:31\t Batch:59\t loss:0.065154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:31\t Batch:79\t loss:0.056994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:31\t Batch:99\t loss:0.038525\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:31\t meanTrainLoss:0.043303\t meanValLoss:0.043283\t meanValIOU:0.738806\t BestValThred:0.550000\n",
      "Epoch:32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:32\t Batch:19\t loss:0.030298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:32\t Batch:39\t loss:0.030991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:32\t Batch:59\t loss:0.038620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:32\t Batch:79\t loss:0.036156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:32\t Batch:99\t loss:0.041706\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:32\t meanTrainLoss:0.040790\t meanValLoss:0.047353\t meanValIOU:0.683831\t BestValThred:0.550000\n",
      "Epoch:33...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:33\t Batch:19\t loss:0.042544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:33\t Batch:39\t loss:0.043289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:33\t Batch:59\t loss:0.091157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:33\t Batch:79\t loss:0.047066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:33\t Batch:99\t loss:0.069586\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.22it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:33\t meanTrainLoss:0.042045\t meanValLoss:0.043095\t meanValIOU:0.733458\t BestValThred:0.500000\n",
      "Epoch:34...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:34\t Batch:19\t loss:0.039131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:34\t Batch:39\t loss:0.025424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:34\t Batch:59\t loss:0.035658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:34\t Batch:79\t loss:0.080643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:34\t Batch:99\t loss:0.041484\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:34\t meanTrainLoss:0.039401\t meanValLoss:0.039669\t meanValIOU:0.742662\t BestValThred:0.550000\n",
      "Epoch:35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:35\t Batch:19\t loss:0.042969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:35\t Batch:39\t loss:0.043009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:35\t Batch:59\t loss:0.033076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:35\t Batch:79\t loss:0.035305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:35\t Batch:99\t loss:0.085112\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:35\t meanTrainLoss:0.040834\t meanValLoss:0.048345\t meanValIOU:0.725622\t BestValThred:0.450000\n",
      "Epoch:36...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:36\t Batch:19\t loss:0.041471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:36\t Batch:39\t loss:0.051271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:36\t Batch:59\t loss:0.032869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:36\t Batch:79\t loss:0.034913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:36\t Batch:99\t loss:0.046958\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:36\t meanTrainLoss:0.039841\t meanValLoss:0.042383\t meanValIOU:0.726617\t BestValThred:0.450000\n",
      "Epoch:37...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:37\t Batch:19\t loss:0.030858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:37\t Batch:39\t loss:0.042508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:37\t Batch:59\t loss:0.020426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:37\t Batch:79\t loss:0.030580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:37\t Batch:99\t loss:0.013561\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:37\t meanTrainLoss:0.036789\t meanValLoss:0.042149\t meanValIOU:0.751493\t BestValThred:0.500000\n",
      "Epoch:38...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:38\t Batch:19\t loss:0.024151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:38\t Batch:39\t loss:0.020756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:38\t Batch:59\t loss:0.036151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:38\t Batch:79\t loss:0.032137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:38\t Batch:99\t loss:0.029790\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:38\t meanTrainLoss:0.036197\t meanValLoss:0.050787\t meanValIOU:0.708458\t BestValThred:0.550000\n",
      "Epoch:39...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:39\t Batch:19\t loss:0.030177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:39\t Batch:39\t loss:0.029723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:39\t Batch:59\t loss:0.030992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:39\t Batch:79\t loss:0.037738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:39\t Batch:99\t loss:0.043427\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:39\t meanTrainLoss:0.035112\t meanValLoss:0.041986\t meanValIOU:0.730473\t BestValThred:0.500000\n",
      "Epoch:40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:40\t Batch:19\t loss:0.032917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:40\t Batch:39\t loss:0.057720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:40\t Batch:59\t loss:0.030004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:40\t Batch:79\t loss:0.060698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:40\t Batch:99\t loss:0.043243\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:40\t meanTrainLoss:0.035902\t meanValLoss:0.046503\t meanValIOU:0.731965\t BestValThred:0.500000\n",
      "Epoch:41...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:41\t Batch:19\t loss:0.029422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:41\t Batch:39\t loss:0.045332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:41\t Batch:59\t loss:0.067185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:41\t Batch:79\t loss:0.042635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:41\t Batch:99\t loss:0.030033\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:41\t meanTrainLoss:0.037742\t meanValLoss:0.046504\t meanValIOU:0.725871\t BestValThred:0.500000\n",
      "Epoch:42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:42\t Batch:19\t loss:0.030891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:42\t Batch:39\t loss:0.023953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:42\t Batch:59\t loss:0.036405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:42\t Batch:79\t loss:0.063172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:42\t Batch:99\t loss:0.032372\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:42\t meanTrainLoss:0.034192\t meanValLoss:0.042986\t meanValIOU:0.743408\t BestValThred:0.600000\n",
      "Epoch:43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:43\t Batch:19\t loss:0.030372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:43\t Batch:39\t loss:0.054101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:43\t Batch:59\t loss:0.063442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:43\t Batch:79\t loss:0.031725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:43\t Batch:99\t loss:0.056199\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:43\t meanTrainLoss:0.034345\t meanValLoss:0.047891\t meanValIOU:0.737313\t BestValThred:0.500000\n",
      "Epoch:44...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:44\t Batch:19\t loss:0.015113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:44\t Batch:39\t loss:0.022621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:44\t Batch:59\t loss:0.033718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:44\t Batch:79\t loss:0.036310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.56s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:44\t Batch:99\t loss:0.021328\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:44\t meanTrainLoss:0.030611\t meanValLoss:0.042053\t meanValIOU:0.749005\t BestValThred:0.550000\n",
      "Epoch:45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:45\t Batch:19\t loss:0.017570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:45\t Batch:39\t loss:0.030368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:45\t Batch:59\t loss:0.022774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:45\t Batch:79\t loss:0.041894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:45\t Batch:99\t loss:0.052282\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:45\t meanTrainLoss:0.032322\t meanValLoss:0.056643\t meanValIOU:0.680846\t BestValThred:0.600000\n",
      "Epoch:46...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:46\t Batch:19\t loss:0.040368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:46\t Batch:39\t loss:0.041909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:46\t Batch:59\t loss:0.028586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:46\t Batch:79\t loss:0.017310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:46\t Batch:99\t loss:0.034076\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:46\t meanTrainLoss:0.031065\t meanValLoss:0.041332\t meanValIOU:0.742413\t BestValThred:0.500000\n",
      "Epoch:47...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:47\t Batch:19\t loss:0.027703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:47\t Batch:39\t loss:0.035049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:47\t Batch:59\t loss:0.043887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:47\t Batch:79\t loss:0.034325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:47\t Batch:99\t loss:0.032907\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:47\t meanTrainLoss:0.034539\t meanValLoss:0.044009\t meanValIOU:0.749378\t BestValThred:0.500000\n",
      "Epoch:48...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:48\t Batch:19\t loss:0.024646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:48\t Batch:39\t loss:0.028184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:48\t Batch:59\t loss:0.039615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:48\t Batch:79\t loss:0.020755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:48\t Batch:99\t loss:0.038761\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:48\t meanTrainLoss:0.030985\t meanValLoss:0.043688\t meanValIOU:0.750622\t BestValThred:0.500000\n",
      "Epoch:49...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:49\t Batch:19\t loss:0.029802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:02,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:49\t Batch:39\t loss:0.043773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:33,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:49\t Batch:59\t loss:0.053561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [02:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:49\t Batch:79\t loss:0.041743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:35,  1.55s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:49\t Batch:99\t loss:0.034069\n",
      "fold validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:11,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\t Epoch:49\t meanTrainLoss:0.031859\t meanValLoss:0.042040\t meanValIOU:0.751617\t BestValThred:0.550000\n"
     ]
    }
   ],
   "source": [
    "n_fold = opt.fold_iter_num\n",
    "for i in range(n_fold):\n",
    "    (train_idx, valid_idx) = next(fold_)\n",
    "BestModel,BestIou,BestThred,train_loss,val_loss,val_iou,val_bestthred = train(n_fold,train_idx, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "0c95359bd204766cf81bece024a7c4b2c8198b70"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19e0b7e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmKAsWBhjjAnKgoUxxpig/n8+gQheTTWhDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19e0763470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztvXt4ZPdZ2P95Z0Yzus5eJa+0Xnt9\nWdurDSRutrkXkkASQxMnLaFNCPklpZCmTRqgBeoESIIhbeGB0j4/XMCAaVogFwIJDvgXJ4Q4UEiC\n14lDIu3aXq9va2ktrXZXM9Jo7u/vj3PO6Gg0l3PmImlG7+d55pHmzDlnvl/pnPN+37uoKoZhGIbR\njMh2D8AwDMPoDUxgGIZhGIEwgWEYhmEEwgSGYRiGEQgTGIZhGEYgTGAYhmEYgTCBYew6RORJEfne\nHTCOoyKiIhLr4nfsiLka/YEJDMPoACLyYRH5gyb72MPb6GlMYBhGj9BNTcQwgmACw9it/GMRmRWR\nyyLy+yIy6H0gIq8XkYdF5IqI/J2IfKfvs/8kIs+KSFpEHhGR7xGR24APAP9SRFZE5JvVXyYi/we4\nBvisu8/P+D5+m4g8LSIXReRnfcd8WEQ+JSJ/ICIp4J0iEhGRO0TkcRFZEpFPish+3zFvF5Gn3M9+\nFsPoICYwjN3K24DXATcANwE/ByAi/wi4B/g3wAHgt4F7RSQhIjcD7wX+saqOucc/qaqfA/4z8AlV\nHVXV51d/maq+HXgaeIO7z6/4Pn4FcDPwPcAHReS477M3Ap8C9gJ/CLwPeBPw3cAUcBm4yx37NPCb\nwNvdzw4AV7fzRzIMPyYwjN3Kb6jqM6p6CfgI8FZ3+48Bv62qX1PVkqp+FMgBLwFKQAKYFpEBVX1S\nVR/vwFh+QVXXVPWbwDcBv8D5iqp+RlXLqrqGI8h+VlXPq2oO+DDwZtdc9Wbgz1X1r93Pfh4od2B8\nhgGYwDB2L8/4fn8KZ0UOcC3wH11z1BURuQIcAaZU9SzwEzgP6QUR+biITNE+F3y/Z4DROuP0xvdp\n39hO4wiyq9w5VPZX1VVgqQPjMwzABIaxezni+/0aYM79/RngI6q61/caVtWPAajqH6nqK3Ae3Ar8\nsntckLLPrZSGrj7mGeD7qsY3qKrPAvP+eYnIMI5ZyjA6ggkMY7fyHhG52nUYfwD4hLv9d4B3i8iL\nxWFERP6piIyJyM0i8moRSQBZYA1ndQ/wHHBURBrdU88B17c57t8CPiIi1wKIyLiIvNH97FPA60Xk\nFSISB+7E7nGjg9jFZOxW/gj4PHDOff0SgKqewvFj/AaOQ/ks8E73mATwX4GLOGakCRxhA/DH7s8l\nEfl6ne/8L8DPueakn2px3P8DuBf4vIikga8CL3bHPgO8x53bvDv+8y1+j2FsQqyBkmEYhhEE0zAM\nwzCMQJjAMAzDMAJhAsMwDMMIhAkMwzAMIxB9Vczs4MGDevTo0e0ehmEYRk/x0EMPXVTV8Wb79ZXA\nOHr0KKdOndruYRiGYfQUIvJUkP3MJGUYhmEEwgSGYRiGEQgTGIZhGEYgTGAYhmEYgTCBYRiGYQSi\n6wJDRG5zW1meFZE7anz+6247zIdF5FG3xr/32TtE5DH39Y5uj9UwDMOoT1fDakUkitM+8jU4VTMf\nFJF7VXXW20dVf9K3/78HbnV/3w98CDiJ0xPgIffYy90cs2EYhlGbbmsYLwLOquo5Vc0DH8fpUVyP\ntwIfc39/HfAFVb3kCokvALd1dbRGy1zJ5PnsN+ea72gYRs/SbYFxmI0tJs+72zbhNoS5DvirMMeK\nyLtE5JSInFpcXOzIoI3wfOYbz/LvP/YNLq3mt3sohmF0iW4LDKmxrV4DjrcAn1JVr4NZoGNV9W5V\nPamqJ8fHm2a2G11iNe/821ayxW0eiWEY3aLbAuM8G3snX8167+Rq3sK6OSrsscY2s+YKjNW8CQzD\n6Fe6LTAeBI6JyHVuj+G34LSX3ICI3AzsA77i23w/8FoR2Sci+4DXutuMHUi24AiMjAkMw+hbuhol\npapFEXkvzoM+CtyjqjMicidwSlU94fFW4OPq6xerqpdE5BdxhA7Anap6qZvjNVpnzRUYq7lSkz0N\nw+hVul6tVlXvA+6r2vbBqvcfrnPsPcA9XRuc0THWTMMwjL7HMr2NjpArlAHTMAyjnzGBYXQE0zAM\no/8xgWF0BC9KasU0DMPoW0xgGB0hWzQNwzD6HRMYRkeo5GGYhmEYfYsJDKMjWB6GYfQ/JjCMjpD1\noqTypmEYRr9iAsPoCJUoqZxpGIbRr5jAMDpCJdPbTFKG0beYwDDaplxW8kXHJJUxk5Rh9C0mMIy2\n8UJqAVbNJGUYfYsJDKNtPIc3mIZhGP2MCQyjbTz/xXA8ahqGYfQxJjCMtvGS9vaPxMnkS/iq1BuG\n0UeYwDDaxkvaOzCaoFhW8qVykyMMw+hFTGAYbeMJjIMjcQAyVh7EMPoSExhG26xVNAxHYFguhmH0\nJyYwjLbxoqQOjCYAi5QyjH7FBIbRNhUNwzVJrViklGH0JSYwjLbJ5jeapMyHYRj9iQkMo228TO8D\nI45JynwYhtGfmMAw2matWsMwgWEYfYkJDKNt1n0YroZhJinD6EtMYBhtky2UiccijA7GANMwDKNf\n6brAEJHbROQRETkrInfU2edfiMisiMyIyB/5tpdE5GH3dW+3x2q0RrZQYmggytBAFDANwzD6lVg3\nTy4iUeAu4DXAeeBBEblXVWd9+xwD3g+8XFUvi8iE7xRrqvqCbo7RaJ+1fInBgQjRiDA0EDUNwzD6\nlG5rGC8CzqrqOVXNAx8H3li1z48Bd6nqZQBVXejymHYUqsr/+epTLGcK2z2UlskWSxXtYiQRDdzX\n+9PfOM/5y5luDs0wjA7SbYFxGHjG9/68u83PTcBNIvK3IvJVEbnN99mgiJxyt7+p1heIyLvcfU4t\nLi52dvRbwPnLa/z8Z77Nfd+e3+6htIyjYTgCYzgeC9TXO1cs8ZOf+CafePCZpvsahrEz6KpJCpAa\n26prX8eAY8ArgauBvxGR56nqFeAaVZ0TkeuBvxKRb6nq4xtOpno3cDfAyZMne66udjrrPFx7uY/E\nWsEvMIJpGN68l9d6V7MyjN1GtzWM88AR3/urgbka+/yZqhZU9QngERwBgqrOuT/PAQ8At3Z5vFuO\nZ+/vZUdxrlD2maRigXwYnqAwgWEYvUO3BcaDwDERuU5E4sBbgOpop88ArwIQkYM4JqpzIrJPRBK+\n7S8HZukzvNV4LzuKHQ3DuZScrnvNhV/KFRQpExiG0TN0VWCoahF4L3A/cBr4pKrOiMidInK7u9v9\nwJKIzAJfAn5aVZeA48ApEfmmu/2/+qOr+gXP3t/L5TSyhRJDcVfDiAfTMFKuScr7aRjGzqfbPgxU\n9T7gvqptH/T9rsB/cF/+ff4O+I5uj2+7qWgYPWyS8vswRhIx0zAMo0+xTO9tpuLD6HENY9AXVhtM\nwyhs+GkYxs7HBMY2463Ge7npUNbn9B6OxwJFSaXWiht+Goax8zGBsc2sR0n15oNTVVkr+BL34lHy\nxTKFUrnhcZ5msVYokS823tcwjJ2BCYxtptc1jEJJKZV1PUoq4RYgbOLH8Psu0maWMoyewATGNtPr\nPgyvedKgT8OA5vPxR0dZpJRh9AYmMLYZr/91r0ZJee1ZvbDaiobRTGD4NAyLlDKM3sAExjbjmaJ6\nVcPwmicNxqo0jCYCcHmtUPF7WKSUYfQGJjC2Gc/ZnS2UKTZxFO9EsgVnzBUNI+5oGM1NUgWu3jfk\n/G6RUobRE5jA2Gb8zu5MoffMUp6G4S9vDkGc3sWKwLB6UobRG5jA2Gb8K/Fe9GOsuQIvUaklFUbD\nGK78bhjGzscExjaTyZVIxJx/Qy/6MbwoqU0aRoMw4aybe3FozyDRiJjT2zB6BBMY28xqvsj4WALo\nTQ3Di5LyN1CCxomInkaxZ2iAPUMDpmEYRo9gAmMbUVUy+VJFYPSihlHtwxiON9cwPCd3cmiA5GDM\nnN6G0SOYwNhGcsUypbIyPupqGD0oMKqjpAaiEeKxSEPh52kUycEYSdMwDKNnMIGxjXir8IqG0YMm\nqeo8DIDRRKyhec3zWTgaxoD5MAyjRzCBsY14dv6KD6MnNQxXYMTXLyWnr3cjDcM1SQ0OkByKWWkQ\nw+gRTGBsI56GMTE2CPSmhpEtlIgIxKPrl9JIPKiGETMNwzB6CBMY24i3Cu9lDWMt7zRPEpHKtuFE\nYw3DS9RzNAzzYRhGr2ACYxvxVuF7hweIRyOBGg/tNPy9MDycvt4NNIxsgXgswuBAlORgjGyhTK7Y\ne3M3jN2GCYxtxFuFD8ejDCeiZHqwiVK2UK7kYHgMx6ON8zDWiiQHBwDH8e1tMwxjZ2MCYxvxHqoj\n8RgjAVub7jScft4bL6ORRKxpWG1yyEnw8wSHmaUMY+djAmMb8QTEcCLKcDzakz6MbKFUycHwGI5H\nmzq9PUGxp6JhmMAwjJ2OCYxtJOPTMIYTsZ6Mkqrpw2iqYRQrpihP07DQWsPY+ZjA2EY8DWNoIMpI\nE7v/TmWtUKrpw8gWnCz2WqTXChXNomKSMg3DMHY8XRcYInKbiDwiImdF5I46+/wLEZkVkRkR+SPf\n9neIyGPu6x3dHutWk8kVGY5HiUSE4Z71YWx2eo/EG7dpTWULJAddH8aQ+TAMo1eIdfPkIhIF7gJe\nA5wHHhSRe1V11rfPMeD9wMtV9bKITLjb9wMfAk4CCjzkHnu5m2PeSlbzpUp115FED/swqjUMX4nz\nMVeD8FBVJ0pqk4bRe3M3jN1GtzWMFwFnVfWcquaBjwNvrNrnx4C7PEGgqgvu9tcBX1DVS+5nXwBu\n6/J4t5RMvljpHzEc71EfRr5GlFSDEue5Ypl8qVwRFIMDEQai0paGcXElx2/81WOU65jAwvDlRxf5\n4unn2j6PqvKbDzzOQirb9rla4Xf/5hzPXMpsy3f3A6u5Iv/9Lx8lX2y/bfLfP3GJv/iH+Q6Mavvp\ntsA4DDzje3/e3ebnJuAmEflbEfmqiNwW4lhE5F0ickpETi0uLnZw6N1nNefTMHo1SqpYQ8NoUOJ8\n2VcWBEBE2i4P8qdfP8+vfv5Rzi6utHwOj//xl4/ykb843fZ5nlrK8MufO8Nnt+FBsbSS45f+4jR/\n8NWntvy7+4UHHlnkv//lY3zj6fYNGr/+hUf5z/e1f03tBLotMKTGtuplYAw4BrwSeCvwuyKyN+Cx\nqOrdqnpSVU+Oj4+3OdytJZMvMuI+XIcTTnZ0J1bJW8lavsRgfHOUFNTWMFK+siAeTnmQ1oXl7FwK\ngMV0ruVzeCyu5HhiabXtAITFlVzHxtTqd8/Op7b8u/uFxbSjGXp/y1ZRVWbnUyymc6j21r1di4Y+\nDBH5540+V9U/bXL+88AR3/urgbka+3xVVQvAEyLyCI4AOY8jRPzHPtDk+3qK1XypEi3kCY61Qqny\nwN3plMtKrljeUNoc1gVGLQ2j0gtjyCcwBmNtaRjeg7Hdh7Oqujc2nLmQ5oXX7mv5XN5YtkVguN85\nM5dCVTfU+TKC0SmBP7ecrWjVqbUie4YHmhyxs2mmYbzBff1r4PeAt7mv3wV+OMD5HwSOich1IhIH\n3gLcW7XPZ4BXAYjIQRwT1TngfuC1IrJPRPYBr3W39Q2ZXJHRxLqGAb3VdS9X3Ng8ycMTfrXmUum2\nN7guFJNDA5WbKizZQonHF1cBWEi35y9I54qVhlCzc8ttnasiMNpcobbCQsr5zkureZ5Lbf339wPe\n33ChTYEx8+z6dbS4sj3+rE7SUGCo6r9S1X+FYwqaVtUfUNUfAE4EObmqFoH34jzoTwOfVNUZEblT\nRG53d7sfWBKRWeBLwE+r6pKqXgJ+EUfoPAjc6W7rGzL5jT4M6K2+3uvNkzZeRp7wqzWX2hpG6xVr\nH7mQruR7tLsa9B/frjnHE17b4fT2C6nZ+fYE326lUxqG/zpa6APhHdT2cVRV/d6753A0gaao6n3A\nfVXbPuj7XYH/4L6qj70HuCfgGHuOVb8PI957Gkaln3coDaOOD6PFsFrvhkzEIh0TGIlYpOIXafdc\nF7dBw1hM54hHI+RLZWbnUrz6lqu2fAy9TqdMirNzKRKxCLlieVu0zU4T1On9gIjcLyLvdBPo/gJH\nGzDaIJMrVVbjow3s/juVSre9TVFSjXwYjmAY22CSirWsYczMLTOWiDE9lWz7hvQeDi+5/gBnLqQp\nlloPqfTOtbSab+s8rX735N5Bjh4YZqZNwbdb6ZTAmJlL8ZLrD3TkXDuBQAJDVd8L/BbwfOAFwN2q\n+u+7ObB+J+/mI6xHSbmr8h4qD7KWry0w4jEnt6JelFTC7YXhkRwcIF8sVwRQGGbnUhyfSjIxluiY\nhvHdN42TK5Y5d3G19XO5wkvV8SVsJYvpHOOjCaankhYp1QKlsrLk/s/aWYQsZwo8e2WNF1+/n3gH\nNOCdQJiw2q8Df6GqPwncLyJjXRrTriBT6YXh+TB6V8OozsMAZ171oqT2DG2MFGm1PEiprJy5kGZ6\nMsl4JwTGSo6BqPCyG50VYTtmqcV0rjLPdh2nob97Jcf4WILpySRPLWVIW9mVUFzO5CmVlT1DAyyt\n5OrWRGuGJ6xPTO1hfLT963MnEEhgiMiPAZ8CftvddBgnusloEa9u1Hqmd+9pGF5EUbUPAxw/xkpN\nDaO4weEN6xFTYf0YTy2tksmXODGVZHx0kMuZQluZuYvpHAdHE9w4Pko8Fml5dV4uKxdX8pyYSjrn\n3WLb9WLaERgnpvYAToiwERzvwX5iKkm5DQ3Ru34qC5pd5MN4D/ByIAWgqo8BE90a1G7AK22+Xkuq\n9zSM9SipGhpGIlYzc315rbAhpBZa1zA8+/z0VLLSF31ptfWb0nvQxqIRbjk0xkyLobXeCnV6Mlk5\n71aRK5ZYXitUTFKwMbTTaI73//L+f62Ga8/MLTMxlmDcfe0aDQPIubWgABCRGDWyro3g1NUwejJK\navNl5JRrr22S2qxhtFbifHY+xUBUODYxVhEY7YQuLri2f3AeFrNu4ltYvJXk8W0QGN53jY8lmBhL\ncGAkbn6MkHgmRE/gtvr/m51LVc6x2wTGl0XkA8CQiLwG+GPgs90bVv9TrWEkYhGikdqO4p1KvSgp\n8HwYtZ3eyaoKtntabKI0O5fixokx4rEIE67AaOemXEznmEi6AmMqyeVMgQst5FF4QuvI/mHGBmPb\nIjAmkglExBzfLeD9DdsR+LliibMLKxUtZWIswdJqnsIWR8x1mqAC4w5gEfgW8G9w8ip+rluD2g1U\nNIz4ehG+4Tqr8p1KI4ExkqinYRQrhQc92tEwTvhWcNC6v6BUVi6trmsY3nlbcXz7V/lbvbKsfPfo\nIOAIvkcvrPT8g2orWUznGIlHufbAsPO+hWvqsedWKJa14keqmExXtjZirtM0FRhuT4v/raq/o6o/\nqKpvdn83k1QbVKKkEusP25E6q/KdSvMoqY1zcXphbNYwPBNVmPIgC+ksi+lcZQV3YDQOtK5hLK3m\nKOv6jX3zoSQitJTH4D1gxscSWx4d4/9ucExr+VKZswvtV/LdLXhRZsPxGKOJ1jTEWZ9/DagsRHrd\nLNVUYKhqCRh3a0EZHcJbfXsaBjjCo5e67q3lnVVrXQ2jai5rhRLFsm7yYQwORInHIqGc3tU3ZCIW\nZe/wQMs3pF8rACeR8uiBkZY1DK/t7lZHx3jz8ARoO5rSbmUxna1cB61qiLPzKYbjUa7dP1w5D/R+\nPamgpUGeBP5WRO4FKtlMqvrfujGo3UBdDaOHfBhrhRLxqON7qWa4xlzWCw9urtjp9MQIPnfPLu/Z\nmYG2VvPVAgOc1fm3Wogw8nwhIsLE2CAPpLeuT8tiOsf+kTgDUWcteN3BUQYHnBDhH9iyUfQ2i+kc\ntxxqz1k9O5fi+GSSiHtvjHfAx7YTCOrDmAP+3N1/zPcyWqSmhhHvLQ0jW9jcbc9jJBEjU9jY32O9\n8ODmdUrY8iCzcymO7B/akATYzmq+2vYPjvby9KVM6HBfL9PaG9NKrrhlpkb/dwNEI8Ith5Ithwjv\nRrzwamhNYJTLusG/BnCwT0xSgTQMVf2Fbg9kt5HJFxkc2Lg6H0nE2i7RvZVkC6WaSXvghNWqOh35\nvEiwWoUHPcJ23ZudS1X8Fx7jYwm+3mKHtIVaGoZ7w5+eS/Fitx5QEBZXchybGN1wvovpPNcc6H6f\nkwXfw85jeirJn39zznpjBCBbKJHKFtcFxmiCvw75kH/mcoaVXHHD9Tk4ECW5xRFz3SBopvdNInK3\niHxeRP7Ke3V7cP2MU6l24wNkOB7tufLmtfwX4Ovv4ZtPrdLmHmG67q3mijyxtMr05J4N2716Ui3l\nTqRzjCViGwTgCfeGDxuWWr1Cha2zXS+mc5UQY4/pySSpbJFnr6xtyRh6mXVNc/3/l84VK3XTglDt\nX/OYSA72fLZ30CXPH+MUH/xdoHeeaDsYp1JtdVnwWE8l7mULm/t5e1T6e+SLgHPzeVFQ1bWkwCkP\ncv5SJtD3nrmQRpUNKj84N3e2UGYlV2SshhbTCC8yxs9EcpCDo4lQDuNsYT3TGtYfPFvRC0FVa87D\n+zvNzKW4et9w18fRy1RHmVU0xJUcR/YH+9vNzqeIRoSbrtpotR8fTfR8T4ygPoyiqv6mqv69qj7k\nvbo6sj5nJVdDw0j0moZRrq9hxGtoGDW67Xk4GkYwk1SlRk8NgQGt2YkX0zkOVj1ove8Io2FcrPPA\n2YqVZSpbJF8sbxIYtxxKEhGLlApCdfBDpYJAiGtqdi7FjeOjm+6Nfqgn1VBgiMh+EdkPfFZE/p2I\nTHrb3O1Gizjd9jZeUKMJR8PolRSXbL6R09uvYTh4Popaq38vSirI3Gfnltk7PMDknsEN2z2HdSsC\n42IN2z845pxHn0sHLmpY/cDZPxInIlvj7KwV6QVOccjrDo5YxncANgmMFpzVM76SIH76oTxIM5PU\nQzg1ozxP2U/7PlPg+m4Majewmi9WmiZ5DMdjlNXplV1v5b6TyBZLHBipnZ6z3kFwow9jyM25qCY5\nFCNfKgeau+fwrnbgtrOaX0zn+K7R2hpGoaROmYcaD4Fa5/GPJRoRDmxR8l61/d3P9NQevv5UawEB\nu4nFdA4RKtf1RMhramklx4VUdlNABjjXRCZfYjVXrBQb7TWa9fS+TlWvB467v1dewPTWDLE/yeQ2\naxgjPdZEaS1f3+ld0TByfg1jc1kQj6DlQYqlMmcupDf5L6B1k9RavkQ6V6ypYVQS3wKuzr0Hy8TY\nuvbTieZOYb673jyevbLGlUxvl6boNosrOQ6MxIm5eSwHRhOOhhiwptjpeaeUfM3rsw9Ca4P6MP4u\n4DYjILWjpHqrxPlaQ6e3Mxd/T4xUdnNZEI+gJc6fuLhKrliuudrfOzRALCKhGxbVM+UAHD0wwtBA\nNLD9vzrT2jvvVtiuF9yHWj3TGoSP+NptLKRylZwJcDTE/SPB/3+z806+y/E6GgZsfX+UTtJQLxKR\nQzjNkoZE5FbWTVNJwMIt2iCTrxUl1VslzrOFMoN18jCGK1FSG01StUJqYT1yqlk9qUoPjKqQWoBI\nRDjYgvnHC3mt9aCNRoRbJoP3xqjOtAZnZfnIFjQxWlzJEY9GakaheQ+w2bkUL7vhYNfH0qvUijIL\n43uYmUsxtWeQfTVMtf2Q7d3MkPY64J3A1cCvsS4wUsAHujes/me1ZpTU5siinUy2UKrZPAnWG0Kt\n5jeapA6O1vZ5BO26NzufIh6LcP34SM3PJ5ItCAyvJHgNgQHO6vzegIlv1ZnW4DwoLq7kKJe1Uiqi\nG3j5H7XG6PXHsEipxlxM57ih6toKIzBm6zi8gY6U4N9umvkwPqqqrwJ+UVVfraqvcl9vBL6xNUPs\nP4quc3e4SmCM9Fib1rVCqWbzJHD6e0SEDWHCjTSMoCap2bkUtxwa27CC99NKPalGJilwejKns0XO\nX26e+FYr03p8LEGhpKGq8bZCvdBgjxPWG6Mhqroh6dIj6DWVLZR4fHGF6anN2i/AvuE40Yj0r8Dw\n8ZYa2z7VyYHsJjKFjd32PNZ9GDtfYBRKZUplrevDEJFNiYi1Spt7BHF6qyozc8s1I1A8WvEXLKZz\nRAQOjNTRMHyJb0HOVUtgQLhY/laopd34mZ5KcnZhpVKW3thIaq1IvlSuqSEurjSvIHDmQpqyUvf6\ndEym8Z4q/1NNszyMW0TkB4A9IvLPfa93AoONjvWd4zYReUREzorIHTU+f6eILIrIw+7rR32flXzb\n7w05tx2Lt+repGFUoqR2/g291qB5koc/EVFVazZP8hjzTFINyoNcSGW5nCk0DG8dH0uwtJKjVA6e\ny7K4kmP/SKJm1V2Am68acxLfmqzO62Vab1V0zMUa3+1nenIPxbJab4w61PNlBdUQPXNfrQgp/7l6\nWcNo5sO4GXg9sBd4g297GvixZid3my/dBbwGOA88KCL3qups1a6fUNX31jjFmqq+oNn39BreqruX\nNYxsvrnA8GsYq/kSpbLW1TAGB6IkYpGGGkalRk8TDaOscGk13/Dh6aeWVuBnKB7l+vHRpvZ/L9O6\n2hcykXQTCrtYT6pYKrPUZM7rmtIyzztc22yym1lIbw6Jdt6va4h7h+u3BZqdX2ZsMMbV+4bq7jM+\n2tvZ3g0Fhqr+GfBnIvJSVf1KC+d/EXBWVc8BiMjHgTcC1QJjV5GpUdocfBpGi2G1s3Mp7nrg7IaS\n4uvnjvHh209sShasxydPPcPhvUO8/MbaETXZgpP5XM8kBa6G4c6lUqm2jg/D+6yRD2N2LoUI3NJI\nYPhW850SGOCsGh984lLT80DtFar/825waTWPan0/DMC1+4cZiUe5+6/P8cAjwXt0TIwl+OAbTtTV\nwKr5zQce57tuOlhpT9ppCqUyv3r/I/zIK67jqmQgQ0cggvz/qutD+amXUFp9rm74kT7yF7McGE3w\n7u++oePn9hPUh/HPRCQpIgMi8kURuSgiPxzguMPAM773591t1fyAiPyDiHxKRI74tg+KyCkR+aqI\nvKnWF4jIu9x9Ti0ubl2jmnZYrdE8CWAwFkWElpsoffYf5rjvW/M8vriy4TUzl+JTD50Plen7a59/\nhN//2yfqfh7EJDUSj1Uc+J6fjuomAAAgAElEQVQgqBXy6ZEcjDWMkpqZS3Ht/uGGQm/dXxB8Nb/Q\nxPYPjlYzt5zl8mr9xLd6mdYj8ShDA9GuCoyFOt/tJxIR3vaSa4lGZNM1Uu/1D+eX+ehXngocFry0\nkuOXP3eGP3no2Y7MqxYPP3OF3/7rc/zZw539jnYEfqmsnJ5P18y/qD7XxZV8zUVdO3z6G3M8vgWm\nxqD56a9V1Z8RkX+G89D/QeBLwB80Oa6WqK3+S30W+Jiq5kTk3cBHgVe7n12jqnMicj3wVyLyLVV9\nfMPJVO8G7gY4efJkTxRh8kxO1RpGJCIMD7TeRGkhlWMyOcjnf/K7N2x/8uIqr/zVBwI/sEpl5eJK\nvuH+lX7edaKkwNFqvHM06rbn0VTDmE/xHU1MKZ45Iehcy2Xl4orTIa8Rld4Y8yleVkfrqpdpLSJd\nt11XQoObzOMD33+cD3z/8cDnfXxxhe/5tS8zO18/XNSPt3ruptnFMw12OkR4MZ0jHotsKo4ZRGA8\nubTKWqHU0H8BzvVZKiuXM3kONFmkBGUhneXiSi7Q/6ddgmoY3l3+/TgP98a6+TrnAb/GcDVO974K\nqrqkqt5/4neAF/o+m3N/ngMeAG4N+L07mkq3vcTm1flwItayD6OWwxXCZ5hezuQplbXhDRLI6R2P\nVrSpdZNU/TVKoyZKqWyBpy9lmt4UB8ccG3PQuS6vFSiUNJCGAY0d343Cc7ud7d2ojlQ7hM109/Zb\n7GIkUEVgdNi040WZVZuUxhIxErFIw/9fvR4Y1XQj23smgG+vUwQVGJ8VkTPASeCLIjIOBLkiHgSO\nich1IhLHCc/dEO0kIpO+t7cDp93t+0Qk4f5+EHg5feL78Mw01VFS4JgvWo2SqmeLH0nEGI4HN4l4\n+zUKJQxqkvL8NZXmSU01jNrC8oxbo6fZDTkcjzGaCN7ZrFH9JT8HRhMcSg42fHAupLMMRKWm2a2d\nfuNBCDqPsHiZ7l7Ji2ZUNIwuztX7jscXVzsaIlxvwRVEQ5ydTzEQFY5NNO5c3Q1/lndNHt8pGoaq\n3gG8FDipqgUgg+O8bnZcEXgvcD+OIPikqs6IyJ0icru72/tEZEZEvgm8DyezHOA4cMrd/iXgv9aI\nrupJPJNTtUkKnAdeyxpGA+dtGJOIt1+jUMKcZ5Jq4vTerGHUFxh7hmJ1v88rzXEiwCqqlbkGedBO\nTyUb5mLUW6F65+9mHsZiOsfYYKwrVY5PTCWZnUsFKj0/M9ddgVEolXnkQpqjB4YplbWjJVfauX9m\n5lIcmxirWYl5w3m60FBrdj7FNfuHGy7GOkXQFq3DwHuA33Q3TeFoG01R1ftU9SZVvUFVP+Ju+6Cq\n3uv+/n5VPaGqz3ezyM+42/9OVb/D3f4dqvp7YSe3U/Gc2rX6YY8mYi1pGKWycmm1vvM2zArXv1+9\nY9YCCIyReIxMvlTJwYD1fItaeCapWg+m2bkUB0fjgR7srcw1kMCYTHJ2sX7iW7MHzpVMgVyxOzk2\nYaLCwjI9uYdUgEz3tXyJc4srDA1ESWWLXUkQfHxxhXypzJtfeDXQWbNUw//faKJhIEWjkiAbztMF\nk9TpGv3tu0VQk9TvA3ngZe7788AvdWVEu4DVfIl4NFJzNeKEoobXMJZWc5QbhFWGsaH796srMPJO\nWG2zxL1SWckVy6TWCgzHo3VLeoCjfRTLWhFGfhyn656mtZwg5FxDCIwTU0lKZeWx52pHozgPnNph\nnl4s/9JKd8qLN8vybofpgCXeH3nOyXR+6Q0HgPXug53EM7+89sQhxhKxjjm+C6UylzL5uvXEGtUo\n85zOzRzeEN483IxKf/stMEdBcIFxg6r+ClAAUNU1akdAGQHI5IubQmo9nGS38CuzZg++Vsw0UH8l\nlA2oYYBTsXa5QVkQj/XyIBsFZr5Y5rHnVgKvosbHEiwGVPkX0lkSsQhjAfJT1h+cte35jTKtu52L\nsZDOdk3D8DLdm5VG8R7er7x5HOjOXGfnUiRiEa4/OMLxqWTgKsLNWFppnMcyPjrI5UyhZufFIAml\nG87VwYi5MxdSaINyJJ0mqMDIi8gQbkisiNwA9G664jazmivV9F+AE1nUSh5GJQ6/zgp3fDTB8loh\nkJlgMZ1j37Dz8K5na/W0gESdFq2wXuJ8NVd0Cw82fih7n1eH1p5dcMwQQVdR42MJ0rkiawEE72La\nCakNorkc2efkgNR6cDbLtO62wFhM5zZlKHeKoJnuM3PLjCVi3HpkH9Cd2lkzbvHJWDTC9GSSMxfS\nocrA1KNZlJn3/1ta3TynmZBO50421KqUIzm8swTGh4DPAUdE5A+BLwI/07VR9TmZfHFTtz2PkUR7\nGkYjlRqCmQkW0zmuHx9tGEqYLZQQcarS1sNf4jy1VgyhYWwUGJ4pJMwKDgLOdSW4KScSEY5PjtV8\ncDbLtO5m85zVXJHVfKlrGgY45rjTTUxSs/Mpjk8lK9dap4WjqlZMk96YMvkSTy2ttn3uRj1R/Ntr\nzWl2PsWR/UOBnc6dDLGemUuxb3iAQx3MeG9E0CipLwD/HCeC6WM40VIPdG9Y/c1qvlTpfVHNcLw1\nH4Z3IR9sskIKchMvpLNMjCUaqs5Zt9teo5X5uoZRalja3KNeifPZuRRDA1GuO1i7B0Y1YarDhnUW\nn5jaw+n51KZM3WaZ1l4l3G5oGBe7FFLrZ3qycYvXUlk5M++0zj0wEkek83OdW86yvLZefDKobyUI\nQUy6/v38nJ5LcaJGQ696dDLE2kuoDKIhd4KgGgbAdwPfA7wK+CfdGc7uIJMrVnpfVDOSiFEoaeho\nmsV0jrFErGbkFTg2WG+/IOcabyIw1gr1+3l7eBpGJu+apBpESEH9Jkozc8vcMjkWuJZRmOqwYQXG\n9GSS1XyJpy9lNp0H6j9w4rEI+4YHuiIwwjjuW6XycK5jlvIynacnk8SiEQ6MxDuuTVX7Co5NjDEQ\nlUBl55vR6oKrFafz+JhjHm43Ys7rb79V/gsIHlb7P4F3A98Cvg38GxG5q5sD62dW86XKw7SaSmvT\nkKG19ZKOPIKaRLKFEqlskfHRRMOV0Fq+3NDhDVUaxlqxYR0pqK1hVMwQIW6KiYBzzRfLXM4UKsI0\nCPV6YzQzCYKXi9H5DOhuZXn7aZbpPlOV6dxKq9xmzMwtIwLHJ53kuHgswo0TtU2EYVlM50g2yGPx\nOkVWz6kVp3On/FnnLq6Sr9PfvlsE1TC+G3idqv6+qv4+TomQV3ZtVH1OJt9Aw4hvbm0ahGbd1g7U\nueCr8Zs3Gtlas8USgw0c3rA+l5VckXQQk1QNH8b5y2uks8VQN8X+gCYRz4EZZmV+7KpRYhHZFCkV\nJNN6YmywOxrGFpikmmW6z85tzHTuRu2s2bkU1x0c2VAhoVNdBBdXcpUy9LVIxKLsHR7YZOYMWhLE\nT6cERiWZtUtVgWsRVGA8Alzje38E+IfOD2d3sJpr4MNww20zIR3fF5uYVgaiEfaPxJtepP4idhNj\ng1xazVMobQ4lzOabm6S8uSymnRyRZk7BeCxSSfry8B4GYW6KikmkyWreiwAL86BNxKLcOLE5YihI\npnW36kktpJyOgftH6vdq6ASNMt1n5zdmOndFYNTQNKcnkyymc21rbgup5sEPtTTu2XnH6Ty5J7iW\nGsY83IjZObe/fUDfXido1nHvs26nuwPAaRF5QES+hFPmY3wrBtiPBNIwQobWOmGV4S/4Wudx9h1c\nDyWskWy25jq9G+GVIb+w7GQINwur9fZZzqxrGDNzKSLi5AKEYTzAaj6IGakWtR6cQXwh3kM0SImN\nMCymcxwcrd8xsFM0ynSvznQO2tY0KMuZAucvr21ayTfzrQSlmUkXagv8mbnwTudKFFmbi4fZ+fUQ\n462i2R38q1syil1Euaxk8qWahQfB58MIoWGs5Uukc8VAF3yzyKHFKpMUOFFTh6pWUNlCfT+Mh9ff\nY37ZWf0FCTtMDm4scT47l+L68dG6zvx6BFnhtmrKmZ5M8qdff3aDkAiSaT0+miBbKLOSKzLWwbo/\nQR52nWDal+n+HVeva3wLKbe8tm/1Pz6aIF8sO76r4fbnWk/TPO7zrbzy5omWzx9U4H/j6SuV957T\n+R0vvTbUdwU1mTZCVZmdS/G6E4daPkcrNOu49+UgJxGRr6jqSzszpP4mU6hf2tzZHl7DCOr0nBhL\n8MTFxjHr3rkO+Oo21bqw1wpl9o80foh7/T0upFyB0cSH4e3jFxin51O88Np9TY+rZnw0wdnnGhem\n8881DP7eGONjblZzgNIQ/r9nRwVGF+tI+Tnhy3T3C4yZ+c12/PUgi2xHBUa1SWrP0ABH9g+1pWGs\n5opkAuSxeBq6qiIiLTudB6IR9g83Nw83Yn7Z6W8fpBxJJ+mULrM1WSN9QKZBaXNne3gNo1nSkUcQ\nM8FiOsf+kTgD0UhDgZEtlAKt+ocTMeauhNEw1rvuXcnkefbKWks3RdC57h0eIBELp714Mfd+Z2vQ\nFaq3byfpZh0pP/Uy3Ws5fsPkwgRhdi61Qev1Mz2ZbEtgBF1wjY8lWCuUKom1lSzrFpzO7fp4WnG2\nd4JOCYye6HS3E6iUNm+mYYSIkgoahz8+5poJ6vScgI3tSuuFEoKXuNf88hmJRyuRV8F8GOsaRjs3\nxfhYomF5dmj9QbtneIDDe4cqD85MvshKQJMgdDbb2+sYuBUaRr1M91qZzmE7HzZjZm657sJhenIP\nTyythvb7eQQ1TVYL/Jm55Zadzu0GQMzOO/3tbz7UmwLDCEij5kngS3YLkYcRRmD49693Lm+/RCzK\nnqGBmhd2kMQ92DjPwD4M9yHvreCb9UmuRaC5tvGgnZ5KMuuGNQZeoYZIKAzKlbUCxbJuicAAZzVf\nnek+W6O8die1qVyxxNmF+sUnp6eSqMKZFntjhL1/FlwTaztO53azvWfnUhw9MNKwv3036JTAsMq1\nAck0aJ4E69Vfw2oYEVkvP1GPIA+s6mireoXSsgGipGCjJtWoF4ZHcihGKlusOPWuSibqZt82Iuhc\nWxYYk0nOXVwlky/6QpEbW2b3Dg8wEJWOFuXbiixvPyem9rCaL/GUm+m+kivy5NIq01WlMZKDMeJN\n2poG5bHnViiWta6meaLNEiFBo+UqWpNr6qwlKIPiBaC0GkU2M7+85eYo6JzAeHuHztP3eIKgXnnz\naEQYGoiG9GHk2D/SPKyymUlEVTetumvZWstlJVsoh9IwRuLRQCux5OAAJTeSbHY+1XJSkhe6WO/h\nrKqVmlmtcMJd1T5yIR1YwxCRjrdq9fIPulWptprqMNZH3EznanNRJ+farPjk5J5B9g4PVDS+sCyk\ns0Qjwr7hxsEPfq3pQqo9p3MQ83A9ltcKPHNpbUtLgng0y8NIi0iqxistIhVxrqrf7v5Q+wPP1FRP\nwwBnVR7GHruQap6DAc3NBKlskXyxvElgVD90c8XmzZM8PA0jSISUf7/FdI7HGpghmtFsriu5ItlC\nuS2TFDhx+GHCczud0LbVGsaNExsz3Rv5mTo119m5FMPxKEcP1PYViEhbjm8njyVOpMmCa+/QALGI\nsJjOMfNse07ndkx2Z2pEpW0VDQWGqo6parLGa0xVt360fUBFw2gQYTTstjYNSlBb/J6hAeLRSN2s\n2FoPH38oocd686TmGoOnYTSrI+Xh+TlOPXWZUgMzRDPGErGG5dnbfdAe3jtEcjDG7HyqYhIMkmnd\n6wJjcGBjpvvMXIq9dTKdOykwjk8mGz7Qvd4YxRpVCZoR1DQZiUilRla7Tud2BEYlJ2WnaRjViMiE\niFzjvbo1qH7GC6ttlPQ2HA+nYQS94EWk4U1cy7RSHUoIvn7eAcJqvYz2oL0CPMHy1XNLQOudxILP\ntTVTjoi4jm9HYBwImGnd6fIgi+kcQwPRupUDuoE/090r11Er07kTAqNcDlZ88sThJLliuWmeUS3C\n9ETx/n/tOp2DFsisxcxcioOjiaY+s24QtFrt7SLyGPAE8GXgSeD/6+K4+pZmYbXOZ8E1jLBhlQcb\nPURrmFZqrYQ8gRHIh+HeUEFCav37feXxJUYTMa7ZPxzouFo0FBgdKNh3YmoPZy6kuJDKBn/gjCZY\nWsl1pEscrGuXW9UPARwhvpDO8Vwqy5kL6bp2/PHRBJcytWuRBeX85TVWcsWmvgLP6d5KqfMwwQ/e\nNeX1oWiVdupJVZdh2UqCahi/CLwEeFRVr8Ppi/G3XRtVH5PJF4lFhHgDB/BwPMpKQA1jea1AoaSh\nHlj1LlIvXLCZwMiGEBhhNQxvv2evrHF8cqypXbkRjebaCVPO9GSSbKHMQ09dDvXAKavToa8TbFWW\ntx/vYfXZb841zHSeSCbQNufqVWRt9oC8fnyEeCwSOlLKWXDVb61bzfhogqeWMjx9KdOW0zk5FCMe\njYQWGPlimccWtrYHhp+gAqOgqktAREQiqvol4AVdHFffsporMRxv3KluJB4L3HUv7Ep5fCxRt3Xp\n4kqOeDSywd/QrsDwfBhhnd7QfmP7RuafxXSOWETYG3BctfAeYuls86Q9/5i87+8EW5Xl7cf7v3zq\nofPu+9qRbJ3IO5mdTxGNCDc1KT45EI1w81Xhe2NczuQplUMsuMYSlcVcO6v8ZibTepxdWKFQat23\n1y5BBcYVERkF/gb4QxH5H0BraZW7nEy+2LRo33AiymrAxL2wK+XxsQRLq/mazkFvteoXZusZu+uO\n8rW8c2yYPIxm3fY8/Lka7db598qz54ub57rgzrUdDebGidGKphg0PHfc/Xt2qpHSQjpXCSHeKvYO\nxzm8d4gzF9LEYxFuGK8dvdQJ4Tg7l+KG8ZFAi5MTU0lm5pZD5TZ4EYBB/QH+v3W7dZxaaai13gNj\nZwuMvwb2Aj8OfA54HHhDkANF5DYReUREzorIHTU+f6eILIrIw+7rR32fvUNEHnNf7wg41h3Nar7U\nMEIKwmkY63H4wQVGPTNBrSZMXijhQg0NI5jACKdhDEQjlb9Pu6uoSnn21c0PrE6YcgaiEW46NLrh\nu5ox0UENI1cssbxW2HINA9b/N40ynf3VjlslTC7O9FSSy5lCpdhlEEIvuCplcxJt5760omHMzjv9\n7euFGHeboAJDgPuBB4BR4BOuiarxQSJR4C7g+4Bp4K0iMl1j10+o6gvc1++6x+4HPgS8GHgR8CER\nCV+2dIexmguoYQR0erd6wddKaKtl3vCHEnqsO72D1JJyBUaI6qzJQUdI3TgxGviYWjRa4XbKlOOZ\nZwIHHXhmmg5ESl10+5RstQ8D1ufdyGx4sE2T1KXVPPPL2cCmyUob2RBmqaBJlx7e37oTJqFG5uF6\nzM6lQvW37zSB7ASq+gvAL4jIdwL/EviyiJxX1e9tcuiLgLOqeg5ARD4OvBGYDfC1rwO+oKqX3GO/\nANwGfCzImMPwzKUMH/j0t2p+FhHh377yBl5y/YGOfFcmF0zDyBfLFEplBppkRy+mcwwORAKH9zV6\niF5cyXHrNZtlcrUvIFSUlOf0Dhgl5e27d3gg0Pkb4c31Zz/9bfZWldh+fHGF77y6/daWFYER8IEz\nFI8ylojxR197mq883nTN1RAv9HpbBIb7wGz04BwciJIcjLUsMMIWn7xlMokI/Of7TvO//u7JQMc8\ne8Vp7hXWB9UJp/P4qGMefvvvfS3wMQ8/c4U3v/Dqtr+7VcIGES8AF4AlIEi3ksPAM77353E0hmp+\nQES+C3gU+ElVfabOsYerDxSRdwHvArjmmtZSQ8qqdaOSTs+n2D8S75jAWM0XOdTEXuovcb5nqLnA\nCBNWWc8kUiyVWVqtHS0yPpbgOZ+anwuRh3Hi8B7e8PwpTh7dH2h8AD/0omvqtrANw81XjfHqWya4\nnMlv+v+emEpy2/Pabz7zuucd4tRTl3ne4eDC54decg1//8SlwJFwjXjFjQd5/pG9bZ8nLC+94QCv\n/85JXjN9VcP92sk78bLJgz6cRxMx3vHSo3zz/JXAf9s9QwP84Auvbqr1exzeO8SbX3g1b7p1KtD+\njXjVLRP83eMXQ10H33F4D2+6ddNjcMsI9FcSkX+Lo1mMA58CfkxVg2gJtZ5i1R6pzwIfU9WciLwb\n+Cjw6oDHoqp3A3cDnDx5sqXg9msPjPDpf/fymp/9yP96sO32j34y+fr9vD0qFWvzxaYZ0mGSjqB+\nPalLq3lUa6+0xkcTfPvZ9To9YTSM0USM//ettwYeH8A7X35dqP3rMRSPcs87/3FHzlWPyT1D/MYP\n/aNQx7z/+453aTRbR3JwINC820nem5lLMbVnkH0hepV/+PYTLX1XUGLRCL/6g8/vyLlecGQvf/zu\nl3XkXFtFUB/GtcBPqOoJVf1QQGEBjlZwxPf+amDOv4OqLqmqd0X9DvDCoMduBSem6vcxboXVXP1+\n3h6ehhEkUiqs83ZwIMpYDTPBQgNbrhdZ5SWbeVFSgzGrjm80ZiJAb/V6bGeCmlGbQHe8qt6hqg+3\ncP4HgWMicp2IxIG3APf6dxCRSd/b24HT7u/3A68VkX2us/u17rYtZXrS6WP8aJN2n0Fp1M/bY9Sn\nYTSjlWifWqu+9RLdm881kUxQKiuXM46TNVssEY9GtrT5vNGbtKphZAslHl9svfik0R26eserahF4\nL86D/jTwSVWdEZE7ReR2d7f3iciMiHwTeB/wTvfYSzgZ5g+6rzs9B/hWUl3OuR1UldV8sWFZEFhP\ndmumYeSLZS5nCqHD+2plQDeKFqlOwFrLl0gEiJAyjPGxBKv5UuhueI9cSFNWmG4zF8foLF1v16Sq\n9wH3VW37oO/39wPvr3PsPcA9XR1gE+r1MW6FbKGMav1uex6eQGmmYVxssR7S+Fhi03waZYz7+zMf\nnwzePMkw/IuNoI5lWK8JtV0JakZtbJnYhEjErbXfYjcvP15p88AaRpNcjLAx5B7jY4lK3Sj/ucYG\nYzUd2dWhuNlCKVCElGG02sd8dn6ZsUSMq/cNdWNYRouYwAjA9NTmPsatEKR5Evg0jCZqfKsF9GqZ\nCRr5QqoTsNYKJQZjJjCM5rRaHmR2LsXxqdpl043twwRGAKYnk2R8fYxbpeMaRqsmKVcA+LNMG2U+\njyRijMSjPoFRZtA0DCMArQiMUlkblk03tg8TGAFYb8fZWs9gj0yl216T0iDxcBrGgdHgceqwXmjN\nfxM369rnT8ByfBh26RjN2T8cJ+q2NQ3Kk0urZPIli5DagdhdH4BjV7l9jNt0fHtRT800jIFohHgs\nwkoTp/diOsfe4QESIc1DtcpONwvPdcIjHb9HtlBqu2yHsTtwapHFQwmMsCVBjK3DBEYAEjG3j3Gb\nju+gGgY4jYcyTcJqWy2gV+2IzOSLrOSKDcNz/QlYFiVlhCFseZDZ+RQDUeHYROMeGMbWYwIjIF7/\n5nZYCej0BkeorDbRMBbS2ZZ6IewfiRORdQ0jiPPcqd2/7vQ2gWEEZXw0XN+HmbkUxybGiFslgR2H\n/UcCcmJqDwvpXFt9DCoaRhOTFDhmq6YaRsg6Uh7RiHBgNMFCKpzASGeLZAsl1vJlEiYwjICEzfa2\nkiA7FxMYAanU2m/DLLXaQQ1DVdtqAjQ+um4mCJLP4fd75EzDMELg9H3IBwpLX0hnubiSM4f3DsUE\nRkBaac5STSZfRCRg46FElEyDsNqVXJFsody6wPCt+oKE5/r9HmuFUqA5GAY4iw1/LbJGzFqG947G\n7vqA7Bke4PDeobY1jJF4LFAy0nA81rD+TqtJex4bBEY6R0Qc30aj/QHmr2QpltU0DCMwXh/zII5v\nryTIcRMYOxITGCHwmsy3SiZfbNptz2Mk3ljDWDcjtdZXeMJtD1kuO6atA6OJhm0fPYHxzGUnedFK\ngxhBCZO8Nzuf4sj+oVAtfY2twwRGCKankjxxcTVQ2fFarOZLgQuwDSdiDb+n1Sxvj/GxBMWycmWt\nECg898BIHBF42s12N6e3EZR6XR5rcXouZf6LHYwJjBBMTyZRhTMXWuuNkckF1zBGE7GG5c0r/Sva\nEBjeeRZXck3Dc2PRCAdG4jzjCgwzSRlBCaphrOaKPLG0ygkrab5jMYERghNu3+ZWHd+r+WKgCClw\nyoOsFUqVLnfVLKRzDESlaQvXevijnhZSwcJzD44mKhqGCQwjKCOJGMPxaCWPpx5nLqRQDd7D29h6\nTGCEYGrPIHuGBlrujZHJl5qWBfHwBMtandawi+kcB0cTRBr4HRrhrfqeSzlhjEFMW+NjCZ69vAYE\ni/QyDI8guRhWEmTnY3d9CETa642xmisyHNiH0bgAYTs5GLAuMB5dSFMsa2CBUXQ1HtMwjDDU6vJY\nzex8ir3DA0zuaS2Qw+g+JjBCMj2V5Mx8imKpHPrYTL7ESOAoqcYlzlutI+UxmogxOBCprOqCCgwP\nK29uhCFIPamZuRQnrAfGjsYERkhOTCXJFcs8cXE19LGruWKgwoOwXuK8Xi5Gs3LkzRARxscSnHa1\npSDCx7+PNVAywtDMJFUslTlzIW3+ix2OCYyQePbVsGYpVQ3nw3BNV7VyMUplZalNgQFOBdqLK072\nbVgNw/IwjDCMjyZYXiuQK9bWmM9dXCVfLJv/YodjAiMkN4yPEo9FQkdK5UtlimXtiIZxaTVPWVsP\nqfXwawxeU6VG+Mufmw/DCIMXtu0tUKqpOLwnLaR2J2MCIyQD0Qg3XzUWWsNYLzwYTsOoVYDQKxXd\nrobhHT80EA00rg0+DIuSMkLgXTsLqdplzmfmlonHItwwPrKVwzJCYnd9C0xPJpmZS6HavPqmh6cp\nBI6SqrRp3azCt1tHysM7fnwsEcjRuFFgmIZhBMcrYVPPjzE7n+KWQ2PEovZI2snYf6cFpqeSXFrN\n81wqeI1/zxcRNHFvPUpqs4bRbh0pD7/ACEJyMEY8FkEEEtbcxghBdZdHP6rq9MAwh/eOp+t3vYjc\nJiKPiMhZEbmjwX5vFhEVkZPu+6MisiYiD7uv3+r2WIOy7vgOXohwNUTzJP9+tZze3k13cKx+ddkg\neD6MoOG5IsL4aILBWH7jjGEAAAwySURBVNRCH41QHBh1rtVaGsaFVJbLmYI5vHuAYMvdFhGRKHAX\n8BrgPPCgiNyrqrNV+40B7wO+VnWKx1X1Bd0cYysc9/XGePUtVwU6JhOieRJAPBohFpGaTu/FdI7R\nRCywA70eYTUMb9962eeGUY+BaIT9I/GaAmPmWeuB0St0VWAALwLOquo5ABH5OPBGYLZqv18EfgX4\nqS6PpyOMJmIcPTAcqkRIRcMI6PQWEYbjUf74ofN85dzShs+eWsq07b+A1gVGPcelYTRifDTBfd+a\n3xQwspjOIQI3HzKBsdPptsA4DDzje38eeLF/BxG5FTiiqn8uItUC4zoR+QaQAn5OVf+m+gtE5F3A\nuwCuueaaTo69IdNTyVACw6vyeihE2YMfecV1PPTU5U3bT0wl+Z5bJgKfpx6HkoP86Cuu4/uedyjw\nMT/04mt4einT9ncbu4//52XX8rlvX9i0fTQR4w3Pn2I0YECIsX10+z9Uy9BdCS0SkQjw68A7a+w3\nD1yjqksi8kLgMyJyQlU3PKVV9W7gboCTJ08GD1tqk+nJJPd96wLpbIGxAM1eZuZSXJVMcDBEOY+f\n+N6b2hliUyIR4edePx3qmFfd3L6gMnYnb3vxtbztxddu9zCMNui20/s8cMT3/mpgzvd+DHge8ICI\nPAm8BLhXRE6qak5VlwBU9SHgcaC7T9AQeDX7g/bGmJ1LWZ1/wzB6mm4LjAeBYyJynYjEgbcA93of\nquqyqh5U1aOqehT4KnC7qp4SkXHXaY6IXA8cA851ebyB8SI6Zp5tHimVLZQ4u7hiYYOGYfQ0XTVJ\nqWpRRN4L3A9EgXtUdUZE7gROqeq9DQ7/LuBOESkCJeDdqnqpm+MNw8RYggMj8UAZ3489t0KprBY2\naBhGT9N1L5Oq3gfcV7Xtg3X2faXv9z8B/qSrg2sDEWF6KlhvjJk5RwuxsEHDMHoZS9dtg+mpJI9e\nWKHQpDfG7HyK0USMI/uGt2hkhmEYnccERhtMTybJl8qcXVhpuN/sXIrjk2Mtt1M1DMPYCZjAaAPP\nxNSo1Hm5rJyetzo5hmH0PiYw2uC6g6NOm9MGfoynLmVYzZcspNYwjJ7HBEYbRCPCLYeSDTWMSmMY\nc3gbhtHjmMBoE6dEyHLd3hiz88vEIsKNE6NbPDLDMIzOYgKjTaYnk6SyRZ69slbz85m5FDdOjFrD\nIcMweh4TGG3SzPE9O5cyc5RhGH2BCYw2ueVQkohQ0/G9mM6xkM5ZhJRhGH2BCYw2GYpHue7gSM1S\n56fnzeFtGEb/YAKjA0xP7alpkvKEiGkYhmH0AyYwOsCJqSTPXlljOVPYsH12PsXhvUPsHW6v97Zh\nGMZOwARGB/A0iGo/xuzcspmjDMPoG0xgdIDjrsDwqtICZPJFzl1cNXOUYRh9gwmMDjA+lmBiLLFB\nwzhzIY2qObwNw+gfTGB0iBNTG0uEeL9bDwzDMPoFExgdYnoqydmFFXLFEuD4M5KDMQ7vHdrmkRmG\nYXQGExgdYnpyD8Wy8thzTm+MGTfDW8R6YBiG0R+YwOgQ/hIhxVKZM/MpK2luGEZfYQKjQ1yzf5iR\neJTZ+RRPLq2SK5YtQsowjL7CBEaHiESE45OO43vGemAYhtGHmMDoINNTSWbnHYERj0asB4ZhGH2F\nCYwOcmIqyUquyOe+fYGbDo0yELU/r2EY/YM90TrI9KTj5H76Usb8F4Zh9B1dFxgicpuIPCIiZ0Xk\njgb7vVlEVERO+ra93z3uERF5XbfH2i7HrholGnHCaE1gGIbRb3RVYIhIFLgL+D5gGniriEzX2G8M\neB/wNd+2aeAtwAngNuB/uufbsQwORLlx3PFbTFtIrWEYfUa3NYwXAWdV9Zyq5oGPA2+ssd8vAr8C\nZH3b3gh8XFVzqvoEcNY9347Gy8c4Pjm2zSMxDMPoLN0WGIeBZ3zvz7vbKojIrcARVf3zsMe6x79L\nRE6JyKnFxcXOjLoN3vGyo3zg+29hbHBgu4diGIbRUWJdPn+tuhha+VAkAvw68M6wx1Y2qN4N3A1w\n8uTJTZ9vNc8/spfnH9m73cMwDMPoON0WGOeBI773VwNzvvdjwPOAB9yaS4eAe0Xk9gDHGoZhGFtI\nt01SDwLHROQ6EYnjOLHv9T5U1WVVPaiqR1X1KPBV4HZVPeXu9xYRSYjIdcAx4O+7PF7DMAyjDl3V\nMFS1KCLvBe4HosA9qjojIncCp1T13gbHzojIJ4FZoAi8R1VL3RyvYRiGUR9R3Xazf8c4efKknjp1\naruHYRiG0VOIyEOqerLZfpbpbRiGYQTCBIZhGIYRCBMYhmEYRiBMYBiGYRiB6Cunt4gsAk+1cYqD\nwMUODaeXsHnvLmzeu4sg875WVcebnaivBEa7iMipIJEC/YbNe3dh895ddHLeZpIyDMMwAmECwzAM\nwwiECYyN3L3dA9gmbN67C5v37qJj8zYfhmEYhhEI0zAMwzCMQJjAMAzDMAJhAgMQkdtE5BEROSsi\nd2z3eLqJiNwjIgsi8m3ftv0i8gURecz9uW87x9hpROSIiHxJRE6LyIyI/Li7vd/nPSgify8i33Tn\n/Qvu9utE5GvuvD/hth7oO0QkKiLfEJE/d9/vlnk/KSLfEpGHReSUu60j1/quFxgiEgXuAr4PmAbe\nKiLT2zuqrvK/gNuqtt0BfFFVjwFfdN/3E0XgP6rqceAlwHvc/3G/zzsHvFpVnw+8ALhNRF4C/DLw\n6+68LwP/ehvH2E1+HDjte79b5g3wKlV9gS//oiPX+q4XGMCLgLOqek5V88DHgTdu85i6hqr+NXCp\navMbgY+6v38UeNOWDqrLqOq8qn7d/T2N8xA5TP/PW1V1xX074L4UeDXwKXd7380bQESuBv4p8Lvu\ne2EXzLsBHbnWTWA4D45nfO/Pu9t2E1ep6jw4D1dgYpvH0zVE5ChwK/A1dsG8XbPMw8AC8AXgceCK\nqhbdXfr1ev/vwM8AZff9AXbHvMFZFHxeRB4SkXe52zpyrXe7p3cvIDW2WaxxHyIio8CfAD+hqim3\nj3xf43apfIGI7AU+DRyvtdvWjqq7iMjrgQVVfUhEXultrrFrX83bx8tVdU5EJoAviMiZTp3YNAxn\npXHE9/5qYG6bxrJdPCcikwDuz4VtHk/HEZEBHGHxh6r6p+7mvp+3h6peAR7A8eHsFRFvsdiP1/vL\ngdtF5EkcE/OrcTSOfp83AKo65/5cwFkkvIgOXesmMOBB4JgbQREH3gLU7TXep9wLvMP9/R3An23j\nWDqOa7/+PeC0qv4330f9Pu9xV7NARIaA78Xx33wJeLO7W9/NW1Xfr6pXq+pRnPv5r1T1bfT5vAFE\nZERExrzfgdcC36ZD17plegMi8v04K5AocI+qfmSbh9Q1RORjwCtxSh4/B3wI+AzwSeAa4GngB1W1\n2jHes4jIK4C/Ab7Fuk37Azh+jH6e93fiODijOIvDT6rqnSJyPc7Kez/wDeCHVTW3fSPtHq5J6qdU\n9fW7Yd7uHD/tvo0Bf6SqHxGRA3TgWjeBYRiGYQTCTFKGYRhGIExgGIZhGIEwgWEYhmEEwgSGYRiG\nEQgTGIZhGEYgTGAYxg5BRF7pVVY1jJ2ICQzDMAwjECYwDCMkIvLDbp+Jh0Xkt90Cfysi8msi8nUR\n+aKIjLv7vkBEvioi/yAin/b6EIjIjSLyl26viq+LyA3u6UdF5FMickZE/lB2Q8Ero2cwgWEYIRCR\n48C/xCnw9gKgBLwNGAG+rqr/CPgyTgY9wP8G/pOqfidOprm3/Q+Bu9xeFS8D5t3ttwI/gdOb5Xqc\nukiGsSOwarWGEY7vAV4IPOgu/odwCrmVgU+4+/wB8KcisgfYq6pfdrd/FPhjt9bPYVX9NICqZgHc\n8/29qp533z8MHAX+b/enZRjNMYFhGOEQ4KOq+v4NG0V+vmq/RjV3GpmZ/LWNStg9auwgzCRlGOH4\nIvBmt9eA1yv5Wpx7yauE+kPA/1XVZeCyiPwTd/vbgS+rago4LyJvcs+REJHhLZ2FYbSArV4MIwSq\nOisiP4fT0SwCFID3AKvACRF5CFjG8XOAU0r6t1yBcA74V+72twO/LSJ3uuf4wS2chmG0hFWrNYwO\nICIrqjq63eMwjG5iJinDMAwjEKZhGIZhGIEwDcMwDMMIhAkMwzAMIxAmMAzDMIxAmMAwDMMwAmEC\nwzAMwwjE/w/Y23PEAuEtiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19e072b860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#画出结果图\n",
    "plot_result(train_loss,val_loss,val_iou,val_bestthred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "f9f3f7801fe62c9dc00dcde036862b18630d14ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fold0_iou0.751617_thred0.550000.pth'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存模型\n",
    "BestModel.save('fold%d_iou%f_thred%f.pth'%(n_fold,BestIou,BestThred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "97f07732403efc410a52cec814e867106df7a24c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "test_data = TestDataSet(test_df.index.tolist(), opt)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "f8390c7736a5493e4cdf02d478fbc758c88bbc6c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:00,  1.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:00,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:01,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:01,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:02,  2.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:02,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "7it [00:03,  2.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:03,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "9it [00:04,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "10it [00:04,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "11it [00:05,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "12it [00:05,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "13it [00:06,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "14it [00:06,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "15it [00:07,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "16it [00:07,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "17it [00:07,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "18it [00:08,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "19it [00:08,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "20it [00:09,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "21it [00:09,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "22it [00:10,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "23it [00:10,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "24it [00:11,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "25it [00:11,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "26it [00:12,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "27it [00:12,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "28it [00:13,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "29it [00:13,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "30it [00:14,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "31it [00:14,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "32it [00:14,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "33it [00:15,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "34it [00:15,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "35it [00:16,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "36it [00:16,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "37it [00:17,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "38it [00:17,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "39it [00:18,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "40it [00:18,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "41it [00:19,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "42it [00:19,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "43it [00:20,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "44it [00:20,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "45it [00:21,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "46it [00:21,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "47it [00:21,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "48it [00:22,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "49it [00:22,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "50it [00:23,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "51it [00:23,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "52it [00:24,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "53it [00:24,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "54it [00:25,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "55it [00:25,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "56it [00:26,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "57it [00:26,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "58it [00:27,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "59it [00:27,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "60it [00:28,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "61it [00:28,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "62it [00:28,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "63it [00:29,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "64it [00:29,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "65it [00:30,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "66it [00:30,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "67it [00:31,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "68it [00:31,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "69it [00:32,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "70it [00:32,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "71it [00:33,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "72it [00:33,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "73it [00:34,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "74it [00:34,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "75it [00:35,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "76it [00:35,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "77it [00:35,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "78it [00:36,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "79it [00:36,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "80it [00:37,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "81it [00:37,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "82it [00:38,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "83it [00:38,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "84it [00:39,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "85it [00:39,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "86it [00:40,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "87it [00:40,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "88it [00:41,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "89it [00:41,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "90it [00:41,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "91it [00:42,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "92it [00:42,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "93it [00:43,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "94it [00:43,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "95it [00:44,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "96it [00:44,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "97it [00:45,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "98it [00:45,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "99it [00:46,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "100it [00:46,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "101it [00:47,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "102it [00:47,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "103it [00:48,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "104it [00:48,  2.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "105it [00:48,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "106it [00:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "107it [00:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "108it [00:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "109it [00:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "110it [00:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "111it [00:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "112it [00:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "113it [00:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "114it [00:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "115it [00:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "116it [00:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "117it [00:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "118it [00:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "119it [00:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "120it [00:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "121it [00:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "122it [00:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "123it [00:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "124it [00:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "125it [00:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "126it [00:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "127it [00:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "128it [00:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "129it [01:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "130it [01:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "131it [01:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "132it [01:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "133it [01:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "134it [01:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "135it [01:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "136it [01:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "137it [01:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "138it [01:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "139it [01:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "140it [01:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "141it [01:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "142it [01:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "143it [01:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "144it [01:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "145it [01:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "146it [01:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "147it [01:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "148it [01:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "149it [01:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "150it [01:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "151it [01:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "152it [01:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "153it [01:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "154it [01:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "155it [01:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "156it [01:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "157it [01:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "158it [01:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "159it [01:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "160it [01:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "161it [01:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "162it [01:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "163it [01:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "164it [01:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "165it [01:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "166it [01:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "167it [01:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "168it [01:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "169it [01:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "170it [01:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "171it [01:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "172it [01:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "173it [01:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "174it [01:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "175it [01:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "176it [01:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "177it [01:22,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "178it [01:22,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "179it [01:23,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "180it [01:23,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "181it [01:24,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "182it [01:24,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "183it [01:25,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "184it [01:25,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "185it [01:26,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "186it [01:26,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "187it [01:27,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "188it [01:27,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "189it [01:27,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "190it [01:28,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "191it [01:28,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "192it [01:29,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "193it [01:29,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "194it [01:30,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "195it [01:30,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "196it [01:31,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "197it [01:31,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "198it [01:32,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "199it [01:32,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "200it [01:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "201it [01:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "202it [01:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "203it [01:34,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "204it [01:34,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "205it [01:35,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "206it [01:35,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "207it [01:36,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "208it [01:36,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "209it [01:37,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "210it [01:37,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "211it [01:38,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "212it [01:38,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "213it [01:39,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "214it [01:39,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "215it [01:40,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "216it [01:40,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "217it [01:40,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "218it [01:41,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "219it [01:41,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "220it [01:42,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "221it [01:42,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "222it [01:43,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "223it [01:43,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "224it [01:44,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "225it [01:44,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "226it [01:45,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "227it [01:45,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "228it [01:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "229it [01:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "230it [01:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "231it [01:47,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "232it [01:47,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "233it [01:48,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "234it [01:48,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "235it [01:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "236it [01:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "237it [01:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "238it [01:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "239it [01:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "240it [01:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "241it [01:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "242it [01:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "243it [01:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "244it [01:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "245it [01:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "246it [01:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "247it [01:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "248it [01:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "249it [01:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "250it [01:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "251it [01:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "252it [01:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "253it [01:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "254it [01:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "255it [01:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "256it [01:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "257it [01:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "258it [02:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "259it [02:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "260it [02:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "261it [02:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "262it [02:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "263it [02:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "264it [02:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "265it [02:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "266it [02:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "267it [02:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "268it [02:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "269it [02:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "270it [02:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "271it [02:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "272it [02:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "273it [02:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "274it [02:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "275it [02:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "276it [02:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "277it [02:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "278it [02:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "279it [02:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "280it [02:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "281it [02:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "282it [02:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "283it [02:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "284it [02:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "285it [02:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "286it [02:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "287it [02:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "288it [02:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "289it [02:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "290it [02:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "291it [02:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "292it [02:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "293it [02:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "294it [02:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "295it [02:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "296it [02:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "297it [02:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "298it [02:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "299it [02:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "300it [02:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "301it [02:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "302it [02:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "303it [02:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "304it [02:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "305it [02:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "306it [02:22,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "307it [02:22,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "308it [02:23,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "309it [02:23,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "310it [02:24,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "311it [02:24,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "312it [02:25,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "313it [02:25,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "314it [02:26,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "315it [02:26,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "316it [02:26,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "317it [02:27,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "318it [02:27,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "319it [02:28,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "320it [02:28,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "321it [02:29,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "322it [02:29,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "323it [02:30,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "324it [02:30,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "325it [02:31,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "326it [02:31,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "327it [02:32,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "328it [02:32,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "329it [02:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "330it [02:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "331it [02:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "332it [02:34,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "333it [02:34,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "334it [02:35,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "335it [02:35,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "336it [02:36,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "337it [02:36,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "338it [02:37,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "339it [02:37,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "340it [02:38,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "341it [02:38,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "342it [02:39,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "343it [02:39,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "344it [02:39,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "345it [02:40,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "346it [02:40,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "347it [02:41,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "348it [02:41,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "349it [02:42,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "350it [02:42,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "351it [02:43,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "352it [02:43,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "353it [02:44,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "354it [02:44,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "355it [02:45,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "356it [02:45,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "357it [02:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "358it [02:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "359it [02:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "360it [02:47,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "361it [02:47,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "362it [02:48,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "363it [02:48,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "364it [02:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "365it [02:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "366it [02:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "367it [02:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "368it [02:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "369it [02:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "370it [02:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "371it [02:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "372it [02:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "373it [02:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "374it [02:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "375it [02:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "376it [02:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "377it [02:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "378it [02:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "379it [02:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "380it [02:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "381it [02:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "382it [02:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "383it [02:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "384it [02:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "385it [02:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "386it [02:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "387it [02:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "388it [03:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "389it [03:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "390it [03:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "391it [03:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "392it [03:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "393it [03:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "394it [03:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "395it [03:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "396it [03:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "397it [03:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "398it [03:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "399it [03:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "400it [03:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "401it [03:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "402it [03:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "403it [03:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "404it [03:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "405it [03:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "406it [03:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "407it [03:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "408it [03:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "409it [03:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "410it [03:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "411it [03:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "412it [03:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "413it [03:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "414it [03:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "415it [03:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "416it [03:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "417it [03:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "418it [03:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "419it [03:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "420it [03:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "421it [03:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "422it [03:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "423it [03:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "424it [03:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "425it [03:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "426it [03:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "427it [03:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "428it [03:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "429it [03:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "430it [03:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "431it [03:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "432it [03:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "433it [03:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "434it [03:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "435it [03:22,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "436it [03:22,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "437it [03:23,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "438it [03:23,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "439it [03:24,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "440it [03:24,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "441it [03:25,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "442it [03:25,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "443it [03:25,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "444it [03:26,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "445it [03:26,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "446it [03:27,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "447it [03:27,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "448it [03:28,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "449it [03:28,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "450it [03:29,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "451it [03:29,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "452it [03:30,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "453it [03:30,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "454it [03:31,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "455it [03:31,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "456it [03:31,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "457it [03:32,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "458it [03:32,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "459it [03:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "460it [03:33,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "461it [03:34,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "462it [03:34,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "463it [03:35,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "464it [03:35,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "465it [03:36,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "466it [03:36,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "467it [03:37,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "468it [03:37,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "469it [03:38,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "470it [03:38,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "471it [03:38,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "472it [03:39,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "473it [03:39,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "474it [03:40,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "475it [03:40,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "476it [03:41,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "477it [03:41,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "478it [03:42,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "479it [03:42,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "480it [03:43,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "481it [03:43,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "482it [03:44,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "483it [03:44,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "484it [03:44,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "485it [03:45,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "486it [03:45,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "487it [03:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "488it [03:46,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "489it [03:47,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "490it [03:47,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "491it [03:48,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "492it [03:48,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "493it [03:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "494it [03:49,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "495it [03:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "496it [03:50,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "497it [03:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "498it [03:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "499it [03:51,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "500it [03:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "501it [03:52,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "502it [03:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "503it [03:53,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "504it [03:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "505it [03:54,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "506it [03:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "507it [03:55,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "508it [03:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "509it [03:56,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "510it [03:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "511it [03:57,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "512it [03:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "513it [03:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "514it [03:58,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "515it [03:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "516it [03:59,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "517it [04:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "518it [04:00,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "519it [04:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "520it [04:01,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "521it [04:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "522it [04:02,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "523it [04:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "524it [04:03,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "525it [04:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "526it [04:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "527it [04:04,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "528it [04:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "529it [04:05,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "530it [04:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "531it [04:06,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "532it [04:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "533it [04:07,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "534it [04:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "535it [04:08,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "536it [04:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "537it [04:09,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "538it [04:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "539it [04:10,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "540it [04:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "541it [04:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "542it [04:11,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "543it [04:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "544it [04:12,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "545it [04:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "546it [04:13,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "547it [04:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "548it [04:14,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "549it [04:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "550it [04:15,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "551it [04:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "552it [04:16,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "553it [04:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "554it [04:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "555it [04:17,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "556it [04:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "557it [04:18,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "558it [04:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "559it [04:19,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "560it [04:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "561it [04:20,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "562it [04:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "563it [04:21,  2.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "#开始测试,数据会保存在test_df里\n",
    "test(BestModel, BestThred, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "7ec42b7dc1bf453bdd0f40ed8f4a9672b067e4cf"
   },
   "outputs": [],
   "source": [
    "test_df = get_fold_rle_mask(test_df, n_fold)\n",
    "test_df.reset_index(inplace=True)\n",
    "submission = test_df[['id','rle_mask']]\n",
    "submission = submission.astype(str)\n",
    "submission.to_csv('submission_fold%d.csv'%n_fold, index=False)\n",
    "test_df.to_csv('test_df_fold%d.csv'%n_fold, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "9c309cfa415580badb061b3574114c6865cfd8d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rle_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>353e010b7b</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5439dbbddf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71bab9f311</td>\n",
       "      <td>2003 9 2103 11 2204 12 2305 12 2406 12 2507 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52551f7a80</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512d8d9997</td>\n",
       "      <td>1 30 102 30 203 31 304 31 405 32 506 33 607 34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           rle_mask\n",
       "0  353e010b7b                                                   \n",
       "1  5439dbbddf                                                   \n",
       "2  71bab9f311  2003 9 2103 11 2204 12 2305 12 2406 12 2507 12...\n",
       "3  52551f7a80                                                   \n",
       "4  512d8d9997  1 30 102 30 203 31 304 31 405 32 506 33 607 34..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db5a4c51303c5c1d7ac1cc1a0a0845d4b123f19f",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
